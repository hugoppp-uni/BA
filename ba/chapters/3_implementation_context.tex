\chapter{Implementation Context}

This chapter goes over the technical background of the implementation and the context in which it is developed.


%\section{Requirements and Constraints}

%\paragraph{Accuracy might vary based on many variables}
%
%\begin{enumerate}
%    \item Object texture
%    \item Distance
%    \item Lighting conditions
%\end{enumerate}
%
%\paragraph{Near-real-time (once every 10s or so)}
%
%\paragraph{Limited processing power (smartphone)}


\section{Potential pitfalls}
During the planning phase of this thesis, a potential pitfall of the system was identified.
The development device, Google Pixel 7, lacks a depth sensor.
Extracting depth information using the Depth API is still possible, however
the Depth API will rely solely on Depth from Motion techniques to derive depth information from camera
images as later described in section~\ref{sec:technical-background-depth-from-motion}.
It is important to note that camera-based Depth from Motion has limitations
when it comes to detecting depth in objects with minimal texture, such as walls.
This drawback could potentially present challenges, especially when detecting walls or furniture with minimal texture,
where the accuracy of depth information obtained from the Depth API may not be sufficient for accurate recognition. \parencite{google_llc_arcore_doc}
This pitfall is addressed in the evaluation, chapter~\ref{ch:evaluation}.

\section{Hard- and Softwarestack}

The mobile application is developed for Android and tested using a Google Pixel 7.
The implementation is carried out in Java/Kotlin using the Android Studio integrated development environment (IDE).
The Google ARCore SDK is used to access depth information about a scene.
The SDK is available by default, and no additional libraries are required.

Algorithms are implemented in C\texttt{++} in the \texttt{procedural-augmented-reality} project provided by Prof. Dr. Phillipp Jenke.
The code of this thesis is integrated into the application and interfaced with Kotlin through a binding layer.

\section{Libraries and external code}
The following libraries and/or publicly available code are used in this thesis:
\begin{itemize}
    \item The RANSAC implementation by \citeauthor{schnabel_efficient_2007} is used for primitive detection
    and is further examined in this thesis~\parencite{schnabel_efficient_2007}
    \item \citetitle{google_llc_codelab_raw_depth} provides a reference implementation for using the ARCore Raw Depth API\@.
    It is used as a basis for unprojecting depth image pixels into world space in this thesis.~\parencite{google_llc_codelab_raw_depth}
    \item The monotone chain implementation in C\texttt{++} to compute the convex hull available on Wikibooks~\footfullcite{noauthor_algorithm_nodate}
\end{itemize}

\section{Testing}

All algorithms implemented in the \texttt{procedural-augmented-reality} project are unit tested using
Google Test, a C\texttt{++} testing framework.
To achieve this, a CMake library target \textit{backend} is defined,
that contains all functionality of the \texttt{procedural-augmented-reality} project.
A second target that contains the tests, \textit{backend-test}, is defined,
which links against \textit{backend} and the Google Test library.

The testing process involves writing individual test cases for each function and class to ensure they
behave as expected under various conditions.
For instance, the integrity of the Octree data structure is verified by recursively checking the bounds of
each node and its children.
Other tests validate the correct insertion and retrieval of points within the Octree,
ensuring that nodes are placed in the correct sub-octants.
Edge cases are also considered, such as testing the behavior of the Octree when searching for points within a radius.
Additionally, the deletion functionality is extensively tested through various scenarios,
including deleting nodes from the root and upwards or downwards in the hierarchy.

