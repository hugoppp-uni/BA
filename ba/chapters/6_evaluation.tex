\chapter{Evaluation}\label{ch:evaluation}
In this chapter, the individual algorithms, as well as the final performance of the combined system, will be evaluated.
The depth from motion algorithm will be evaluated based on different materials and their resulting confidence maps in section~\ref{sec:eval-depth-from-motion}.
In section~\ref{sec:eval-ransac-algorithm}, the RANSAC algorithm will be evaluated based on synthetic data, as well as real world data.
Finally, in section~\ref{sec:eval-combined}, the combined system performance will be evaluated.
%klare begründung => Anforderung erfüllt ja oder nein

\section{Depth from Motion}\label{sec:eval-depth-from-motion}
As the depth from motion algorithm greatly differs in its performance based on the amount of texture in the captured scene~\cite{google_llc_arcore_doc},
this section will evaluate different materials based on their resulting confidence maps, as retrieved from the ARCore Raw Depth API\@.
In figure~\ref{fig:materials}, the confidence maps of different materials are shown - the camera image on top, the confidence map on the bottom.

\begin{figure}[h!t]
    \centering
    % First row
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wall-far-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wall-close-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/carpet-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wood-cam}
    \end{subfigure}%

    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wall-far-conf}
        \caption{Wall, 1.5m}
        \label{fig:material-wall-far}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wall-close-conf}
        \caption{Wall, 0.5m}
        \label{fig:material-wall-close}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/carpet-conf}
        \caption{Carpet}
        \label{fig:material-carpet}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/wood-conf}
        \caption{Wooden floor}
        \label{fig:material-wood}
    \end{subfigure}%

    \vspace{0.5em}

    % First row
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/leather-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/whiteFurniture-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/tiles-cam}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/giftpaper-cam}
    \end{subfigure}%

    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/leather-conf}
        \caption{Leather armchair}
        \label{fig:material-leather}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/whiteFurniture-conf}
        \caption{White furniture}
        \label{fig:material-whiteFurniture}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/tiles-conf}
        \caption{Bathroom tiles}
        \label{fig:material-tiles}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/materials/giftpaper-conf}
        \caption{Gift paper}
        \label{fig:material-giftpaper}
    \end{subfigure}%

    \caption{Comparison of confidence maps of different materials. Camera image on top, confidence map on bottom.}
    \label{fig:materials}
\end{figure}

In figure~\ref{fig:material-wall-close} and~\ref{fig:material-wall-far}, the confidence maps of a wall at different distances are shown.
It is apparent that the confidence of the wall at 1.5m distance is almost completely black, meaning low confidence.
This is due to the lack of any texture on the wall at this distance.
When moving closer to the wall, the structure of the wall becomes more apparent and the confidence increases greatly.

In figure~\ref{fig:material-carpet}, a carpet with a lot of texture is shown, the resulting confidence map has fairly high confidence on average,
but also has dark sports throughout the map.
This could be a result of the repetitive pattern of the carpet.

The highest confidence of all tested materials is achieved on a wooden floor, as shown in figure~\ref{fig:material-wood}.
This can be attributed to the high amount of texture and value variance through the wood, while not being repetitive.
The seams between the wooden planks are also clearly visible.

Figure~\ref{fig:material-leather} shows a leather armchair, which has a high confidence on the seams of the chair and
on creases of the leather, with low confidence in other areas, where the leather is smoother.
A similar pattern is visible on white furniture and bathroom tiles, in figures~\ref{fig:material-whiteFurniture} and~\ref{fig:material-tiles} respectively,
where the white surface has very low confidence, while the seams between the surfaces have increased confidence.
The gift paper shown in figure~\ref{fig:material-giftpaper} shows a similar effect, but less pronounced.
As the surface itself is striped with dots randomly placed throughout, the confidence is higher overall,
but the striped pattern is still visible in the confidence map, as the stripes themselves contain little texture.

These examples show how the amount of texture on a given surface greatly influences the confidence of the depth from motion algorithm, as expected.
The algorithm performs best on surfaces with high texture, while struggling with low texture surfaces.
Small repetitive patterns can also lead to low confidence, as visible in the carpet example.
It is also important to note that the texture captured by the camera itself is important, not the texture of the material itself.
This can be seen in the first example of the wall: When capturing from afar, the cameras resolution is too low to capture the texture,
but when moving closer to the wall, the texture of the wall becomes more apparent and the confidence increases.
Lighting conditions will also influence the confidence of the algorithm,
as angled light will create shadows that will increase the texture of the surface~\cite{google_llc_arcore_doc} (not shown in the comparison).

\section{Point clouds}\label{sec:point-clouds}
This section will evaluate the two point cloud datastructures as described in section~\ref{subsec:inserting-new-points-into-the-point-cloud}.
To evaluate the datastructures, the point cloud is transferred from the mobile application to a computer using
a simple HTTP server with a single endpoint, that accepts a file using a POST request and saves it to disk.
The android application will then stream the point cloud data, serialized as a PLY file, to the server on a push of a button.
A PLY file is a simple file format for storing 3D point cloud data in plain text
and allows for saving custom properties for each vertex, such as the confidence value of the point.
In all the following images of the point clouds, the confidence value is color-coded,
with green being high confidence and red being low confidence.

\begin{lstlisting}[caption=Example PLY file]
ply
format ascii 1.0
element vertex 6
property float x
property float y
property float z
property float confidence
property uchar red
property uchar green
property uchar blue
end_header
0.1851168 -0.77074146 -0.92582846 1.0 0 255 0
-0.061887983 -0.7755803 -0.79551744 1.0 0 255 0
-0.058628604 -0.7638883 -0.7860476 0.80784315 245 10 0
-0.07720285 -0.75610626 -0.80394375 0.81960785 230 25 0
-0.19774273 -0.6645372 -2.5003948 1.0 0 255 0
-0.09499544 -0.028413996 -2.553247 0.99607843 5 250 0
\end{lstlisting}

%\begin{lstlisting}[caption=HTTP server code, language=Python]
%from fastapi import FastAPI
%from starlette.requests import Request
%
%app = FastAPI()
%
%@app.post("/upload")
%async def upload(request: Request):
%    body = await request.body()
%    body_str = body.decode('utf-8')
%    with open("data.ply", "w") as f:
%        f.write(body_str)
%\end{lstlisting}

\begin{figure}[h!tp]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/test-setup}
        \caption{Color image of setup}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/test-setup-cloud}
        \caption{Captured point cloud}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/test-setup-segmented}
        \caption{Segmented point cloud}
    \end{subfigure}%
    \caption{Test setup for capturing point clouds}
    \label{fig:test-setup}
\end{figure}

For test data, a cube wrapped with the same gift-paper as in figure~\ref{fig:material-giftpaper} is used.
The bottom of the cube is hollow, so in total, 5 faces are visible.
The cube is propped up on stand, to allow for easy segmentation of the point cloud later on,
in order to remove the floor and other objects from the point cloud.
Reproducibility is ensured by using controlled lighting conditions and a fixed setup.
To capture a scan of the cube, the camera is moved around the cube 3 times, while also moving the camera up and down,
to capture multiple angles for the depth from motion algorithm to work to its full potential.
The full setup is shown in figure~\ref{fig:test-setup}.

\subsection{Quantization Octree vs. Epsilon Octree}\label{subsec:quantization-octree-vs.-epsilon-octree}

A point cloud with a resolution $s$ of 0.005m or 5mm is captured for both the quantization octree and the epsilon octree.
The resulting segmented point cloud, the quantization octree has a total of about 90.000 points,
while the epsilon octree only has about 20.000 points.
This can be explained due to the quantization octree always ending up with the maximal resolution
with enough data added, as it is functionally a grid.
The epsilon octree, on the other hand, will only add points that are within the epsilon threshold of the current point,
which means that gaps between points are at least epsilon wide, but can be wider.
For example, if two points are exactly 2 epsilon apart, no points will be added between them.
As epsilon is set to $s$, the epsilon octree will have fewer points than the quantization octree with the same resolution.
To compare the two datastructures, the quantization octree will be used with a lower resolution of $s=0.010$m,
which results in a number of points of about 22.000, which is comparable to the epsilon octree.

Figure~\ref{fig:octrees-compairison} shows the raw and segmented point clouds of both datastructures.
From inspecting the raw point cloud visually, it is apparent that the quantization octree has a higher amount of noise,
While the cube is easily recognizable in the epsilon octree point cloud, the quantization octree point cloud has a lot of noise around the cube.
The quantization octree also has more low-confidence points than the epsilon octree.
This is due to the quantization octree always adding points to the cloud that fall within the threshold,
while the epsilon octree only adds / updates points based on a condition that takes the confidence of the point into account.
As such, the epsilon octree improves the accuracy of the point cloud when new, higher confidence points are added.


\begin{figure}[h!tp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-clouds-quant2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-clouds-epsilon}
    \end{subfigure}%

    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-clouds-quant-segmented2}
        \caption{Quantization Octree, $s=0.01$m}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-clouds-epsilon-segmented}
        \caption{Epsilon Octree, $s=0.005$m}
    \end{subfigure}%

    \caption{Comparison of Quantization Octree and Epsilon Octree. Raw point cloud on top, segmented on bottom.}
    \label{fig:octrees-compairison}
\end{figure}

To calculate statistics on the point clouds, the real box is measured and a primitive box with matching dimensions is created in CloudCompare.
Using the \textit{fine registration (ICP)} feature, it is possible to align the point clouds to the primitive box.
This allows calculating the cloud/mesh distance between the points of the point cloud and the primitive box.
The result of this calculation can be seen in figure~\ref{fig:octrees-compairison-stats}.
From the histogram, it is apparent, that the quantization octree has higher noise than the epsilon octree.
This can also bee seen in the standard deviation, which is 0.022 for the quantization octree and 0.011
for the epsilon octree.
This confirms the first suspicion from visual inspection of the point clouds.
The mean distance is also higher for the quantization octree, with 0.0046 compared to 0.0011 for the epsilon octree.
This can be understood as points being 0.5cm and 0.1cm away from the real surface in mean, respectively.
If these points were to be used for primitive detection, this would indicate how far off the detected primitives would be from the real surface.

These results suggest that the epsilon octree is an improvement over the quantization octree in terms of accuracy,
both when it comes to the amount of noise and the distance to the real surface.

\begin{figure}[h!tp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-quant-hist}
        \caption{Quantization Octree}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{images/eval-epsilon-hist}
        \caption{Epsilon Octree}
    \end{subfigure}%
    \caption{Histogramm of distances between point cloud points and primitive mesh}
    \label{fig:octrees-compairison-stats}
\end{figure}

\section{RANSAC Algorithm}\label{sec:eval-ransac-algorithm}
In this section, the RANSAC algorithm will be evaluated based on a model of a cube,
first using synthetic data in section~\ref{subsec:tests-on-singular-synthetic-cube} and then using real world data
in section~\ref{subsec:tests-on-real-world-data}.

\subsection{Tests on singular synthetic cube}\label{subsec:tests-on-singular-synthetic-cube}

A unit cube will be used as a test object to evaluate the RANSAC algorithm.
The cube has been generated with a side length of 1 and a sampling rate of 0.01.
This would equate to a cube with a side length of 1m and a distance of 1cm between points in the real world,
which is comparable to what was used in section~\ref{subsec:quantization-octree-vs.-epsilon-octree}.

\subsubsection{Resilience to Noise}
In figure~\ref{fig:test-noise}, the unit cube is shown with varying noise levels using gaussian noise.
The noise level is defined as the standard deviation of the noise added to the points.

With a noise level of 0.01, the cube is perfectly reconstructed.
Increasing the noise level to 0.02, the cube is still recognized mostly correctly.
Starting from noise level to 0.03, the default parameters do not yield correct results.
To achieve correct results, the epsilon parameter of the RANSAC algorithm has to be increased to 0.2.
This leads to 6 faces being recognized correctly, but some points not being assigned to the correct faces,
as the with each primitive extraction pass, points are extracted that lie within 0.2 of the recognized plane.


\begin{figure}[h!tp]
    \centering
    % First row
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/cube_points}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_01}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_02}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_03}
    \end{subfigure}%

%    \vspace{0.5em}

    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/cube_points_primitives}
        \caption{Noise level 0.00}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_01_primitives}
        \caption{Noise level 0.01}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_02_primitives}
        \caption{Noise level 0.02}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/noise/cube_points_noise_03_primitives}
        \caption{Noise level 0.03}
    \end{subfigure}%

    \caption{Unit cube with varying noise level}
    \label{fig:test-noise}
\end{figure}

\subsubsection{Resilience to Missing Data}

As a big problem of depth from motion techniques is the lack of depth information in areas with minimal texture,
the resilience to missing data is crucial.
In figure~\ref{fig:test-missing}, points towards the center of the cubes surfaces have been removed.
This mimics the structure of real world data from the Depth API,
as edges are often detected more accurately than surfaces.
%TODO: citation needed
To achieve this, points have been removed with an increasing probability based on the
quadratic distance to the center of the face the points belongs to.

The algorithm is able to correctly recognize the faces of the cube with a missing level up to 24.
With a missing level of 48, the algorithm still detects the planes, but doesn't assign all points to the faces.
With increasing missing level, the n parameter, which defines the minimum number of points required to fit a primitive,
is also required to be lowered.
In real world applications this would lead to more false detections, primitives being detected where there are none.

%Given a point $x$ with coordinates $(x_1, x_2, x_3)$, edge length $l$, and missing data rate $r$,
%the distance to the closest edge can be calculated as |
%
%As the cube is centered at the origin and aligned with the axis, the absolute of one coordinate will always equal $l/2$.
%By filtering out this coordinate, the point $x$ can be projected onto the plane of the cube,
%resulting in point $x'$ with coordinates $(x'_1, x'_2)$.
%
%The probability $p$ of a point being removed can then be calculated as

%\begin{equation}
%    p = \left(\frac{\sqrt{(l/2 - |x'_1|) \cdot (l/2 - |x'_2|)}}{l / 2}\right)^2 \cdot r
%\end{equation}

\subsubsection{Resilience to Noise and Missing Data}

When combining both noise and missing data, the quality of the detection is much worse.
In figure~\ref{fig:test-both}, the cube is shown with a noise level of 0.01 and varying missing levels.
With noise level 0.01 and missing level 6, the cube is still recognized correctly.
With missing level 12, more than 6 faces are being recognized, but all the points are still assigned to primitives.
This is explained by the fact that the parameterization of the planes is not accurate enough to fit all the points of the planes.
Thus, the algorithm only fits a subset of the points to the primitive and will create a new parameterization
for the remaining points.
To achieve results with these conditions, the epsilon parameter had to be increased to 0.015 to achieve any results at all.
Using a noise level of 0.02 with missing data yields no satisfactory results, even with tweaking the parameters.

\begin{figure}[h!tp]
    \centering
    % First row
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_6_0}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_12}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_24}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_48}
    \end{subfigure}%

%    \vspace{0.5em}

    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_6_primitives}
        \caption{Missing level 6}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_12_primitives}
        \caption{Missing level 12}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_24_primitives}
        \caption{Missing level 24}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/missing/cube_points_missing_48_primitives}
        \caption{Missing level 48}
    \end{subfigure}%

    \caption{Unit cube with varying missing level}
    \label{fig:test-missing}
\end{figure}

\begin{figure}[ht!]

    \centering
    % First row
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m6_n01}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m12_n01}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m24_n01}
    \end{subfigure}%

%    \vspace{0.5em}

    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m6_n01_primitives}
        \caption{Missing level 6}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m12_n01_primitives}
        \caption{Missing level 12}
    \end{subfigure}%
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{python/plots/cube_points/data/matrix/cube_points_m24_n01_primitives}
        \caption{Missing level 24}
    \end{subfigure}%
    \caption{Unit cube with noise level = 0.01 and varying missing level}
    \label{fig:test-both}
\end{figure}

\subsection{Tests on real world data}\label{subsec:tests-on-real-world-data}

