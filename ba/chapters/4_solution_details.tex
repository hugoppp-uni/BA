\chapter{Solution Details}

Depth information is collected using the ARCore Raw Depth API via depth images and confidence images.
Low confidence points under a certain threshold are filtered out.
Each pixel of the depth image is then projected into three-dimensional world-space, utilizing the camera's intrinsics.
An octree is then use to remove duplicate points as new data arrives.
The RANSAC algorithm is then applied to the point cloud to detect geometric primitives.
%todo


\section{Capturing Depth Images}

\subsection{ARCore Depth APIs}


Google ARCore provides two APIs to access depth information: the Depth API and the Raw Depth API\@.
Both APIs work by estimating depth information from a sequence of monocular camera images using depth-from-motion techniques.
Depth data becomes available when the user moves their device and Google claims that the algorithm
can get accurate results from 0 to 65 meters away, with best results between 0.5 and 5 meters.
The technical details of are further discussed in section~\ref{sec:technical-background-depth-from-motion}.

Both the Raw Depth API and Full Depth API provide depth information for a given frame of a camera image using depth images, but they differ in the level of detail they provide:

The Raw Depth API provides depth images and confidence images, where some pixels may not have any depth information.
The depth image provides the distance from the camera of a given pixel in millimeters.
The confidence image indicates the reliability of the depth information for each pixel, ranging from 0 (no confidence) to 255 (high confidence).

In contrast, the Full Depth API provides a single depth image, where each pixel has a depth value.
To achieve this, values for pixels without depth information are interpolated.
No confidence image is provided.

\begin{figure}[ht!]
    \centering
    % First row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_full-depth-image}
        \caption{Full Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_camera-image}
        \caption{Camera image}
    \end{subfigure}%

    % Optional: Adjust or remove vertical spacing between the rows
    \vspace{0.5em}

    % Second row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-image}
        \caption{Raw Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-confidence-image}
        \caption{Raw Depth API confidence image}
    \end{subfigure}%

    \caption{Full Depth vs. Raw Depth}
    \label{fig:depth-api-images}
\end{figure}


The Depth API is preferred in cases where it is crucial to have a depth value for every pixel, such as calculating if an object should be occluded by the scene in an AR application,
while the Raw depth API is preferred if accuracy of the depth information is crucial.
As accuracy is crucial for primitive detection and depth information for every pixel is not required, the Raw Depth API is used in this project.

\parencite{google_llc_arcore_doc}

\subsection{Technical Background: Depth from Motion}\label{sec:technical-background-depth-from-motion}
Depth from motion is a technique developed by Google that estimates depth information from a sequence of monocular camera images
and is used by the ARCore Depth API to provide depth information.
Its primary purpose is to enable AR applications that rely on depth information on devices lacking a dedicated depth sensor or numerous cameras.
This sections provides a brief overview of the workings of the Depth from Motion system by ~\parencite{valentin_depth_2018}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{images/DepthFromMotion}
    \caption{Overview of the Depth from Motion System}
\end{figure}
When users move their smartphones, the system uses ARCore's visual-inertial odometry (VIO)
to determine its position and orientation in six dimensions (6DoF): up/down, left/right, forward/backward, and tilt/swivel/rotate.
After activating tracking and acquiring the most recent camera image (in black and white for faster processing),
a reference image or keyframe from the past is chosen to compare with the current image.

A process known as polar rectification then aligns the keyframe and current frame onto the same plane based on their differences in position and orientation.
This alignment simplifies the process of finding matching points in both images by focusing on comparable horizontal lines.

Next, they use an image correspondence algorithm to identify matching points,
which generates disparity maps that indicate the positional differences between matched points in the two images.
As scenes may contain low textured objects or repetitive patterns which can lead to incorrect matches,
they use an invalidation step, which removes points that are likely to be incorrect matches.
This step also provides a confidence value, which is later used in the interpolation step.
Triangulation is used to compute a sparse depth map based on the disparity maps.

The missing values in the sparse depth maps are then interpolated using a variation of an algorithm called the bilateral solver,
which yields an intermediate data structure known as a bilateral depth grid.
This grid is a structured representation of depth information that can be converted into a full depth map on demand.
assessable through the Full Depth API\@.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral1}
        \caption{Raw point cloud}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral2}
        \caption{Interpolated}
    \end{subfigure}%
    \caption{Interpolation with Bilateral Solver}
\end{figure}

From the paper it is unclear, which part of the pipeline are omitted in the Raw Depth API,
but it is reasonable to assume that the Raw Depth API accesses the sparse depth map and confidence values before the interpolation step.

\cite{valentin_depth_2018}

\section{Building the point cloud (PC)}
As RANSAC is a point-based algorithm, we first need to convert the depth image to a point cloud.
The process consists of three steps.
\begin{enumerate}
    \item Filtering low confidence points
    \item Transforming depth image pixels to world coordinate points
    \item Inserting new points into the point cloud
\end{enumerate}

\subsection{Filtering low confidence points}
The confidence image provided by the Raw Depth API can be used to filter out points with low confidence.
A threshold value is set, below which points are discarded.

\subsection{Transforming depth image pixels to world coordinate points}
To convert the point into world coordinates, first the camera intrinsics are used to
project the point into camera space.
Then, the model matrix is used to transform the point into world space.
\parencite{google_llc_codelab_raw_depth,google_llc_arcore_doc}

\subsubsection{Coordinate Systems and Basis Change}

In computer graphics, different coordinate systems (also known as spaces) are commonly used to represent points in space to simplify calculations.
This section will provide a brief mathematical overview of coordinate systems and how to transform points between them.
As examples, the world and camera space will be used, as they are required to unproject the depth image into world space.
The usage of these coordinate systems in context of the OpenGL rendering pipeline will also be covered in section~\ref{sec:coordinate-systems}.

In the context of linear algebra,
a basis of a $n$ dimensional vector space is a set of $n$ vectors that are both linearly independent
and capable representing any geometric vector in the vector space as a linear combination of these basis vectors.
If the basis vectors are orthogonal and normalized,
they are called an orthonormal basis (provided that the inner product is defined on the vector space).
Combined with an origin $O$ it defines a cartesian coordinate system.
Using that coordinate system, any point $P$ in the vector space can be represented with a $n$ dimensional vector$\vec{p}$ relative to the origin $O$.
In the case of a three-dimensional space and basis vectors $\vec{u}, \vec{v}, \vec{w}$, this is expressed as:
\begin{equation}
    P = O + a\vec{u} + b\vec{v} + c\vec{w} = O + \vec{p}
\end{equation}


Transformation matrix..

Homogenous coordinates..

Basis chagne..

\parencite{dorner_virtual_2019}

\subsubsection{Perspective Projection Matrix}

\subsubsection{Applying the transformation}
"Given point $A$ on the observed real-world geometry and a 2D point a representing the same point in the depth image,
the value given by the Depth API at a is equal to the length of $CA$ projected onto the principal axis.
This can also be referred as the z-coordinate of $A$ relative to the camera origin $C$." \parencite{google_llc_arcore_doc}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{images/depth-values-diagram}
    \caption{}
    \label{fig:}
\end{figure}

The intrinsics can be retrieved from the API using \texttt{frame.getCamera().getTextureIntrinsics()} method.
However, this method returns the intrinsics of the camera image, which differs from the depth image, as the depth image usually has a lower resolution.
To calculate the focal length $f$ and camera center $c$ from the provided intrinsics $I$ and the depth image $D$,
the provided focal length and principal point are scaled by the ratio of the dimensions of the depth image and the camera image:
\begin{equation}
    R = \begin{bmatrix}
            \frac{\dim(D_x)}{I_{xDim}} & 0\\[8pt]
            0                          & \frac{\dim(D_y)}{I_{yDim}}
    \end{bmatrix}\\
    f = R \cdot I_f \\
    c = R \cdot I_c
\end{equation}

These values are then used to unproject the point into camera space (homogenous coordinates):
\begin{equation}
    p_c = \begin{pmatrix}
              d \cdot (x - c_x) / f_x \\
              d \cdot (c_y - y) / f_y \\
              -d                      \\
              1
    \end{pmatrix}
\end{equation}

As the resulting point is in camera space, it is relative to the camera's origin.
To transform it into world space (relative to the worlds origin) the model matrix is retrieved by calling
\texttt{cameraPoseAnchor.getPose().toMatrix(modelMatrix, 0)}.
By multiplying the model matrix $P_m$ with $p_c$, the point is transformed into world space.
\begin{equation}
    p_w = P_m \cdot p_c
\end{equation}

\subsection{Inserting new points into the point cloud}

The last step is to add new points to the point cloud.
As new depth images are captured every frame, we can not simply store a list of all points to append the new data to.
This would lead to a massive amount of points that represent the same point in the real world, with slightly different position values.
To circumvent this, we can use a spatial data structure, that allows us to partition the space into smaller regions, and only store one point per region.
A simple approach would utilize a grid, where each cell represents a region in space, while a more sophisticated approach
would use a spatial data structure like an octree to improve performance.

\paragraph{Octree}
An octree is a tree data structure where each node has exactly eight children.
The tree can be used to sparsely partition three-dimensional space into smaller cubes and allows for efficient
insertion and traversal in logarithmic time.
"For the definition a simple recursive splitting of [cubes] is continued until there is only one point in a [cube]."
\parencite{gabriel_zachmann_geometric_2002}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{images/octree}
    \caption{Octree}
    \label{fig:octrree}
\end{figure}

\subsubsection{Fixed Depth Octree}
A straightforward approach to utilize an octree for point cloud storage is to use a variation of the octree with a fixed depth.
Points are always inserted at the defined depth of the tree, creating all nodes up to that depth if they do not yet exist.
The center coordinate of the leaf node can then be inferred as the point in space,
thus saving the coordinates of the point explicitly is not required.
Instead, to find the coordinates of a given node, the tree needs to be traversed while keeping track of
the extent and center coordinate of the current node.
This approach will naturally provide quantization of the point cloud, with the resolution of the point cloud
determined by the depth of the octree and extend of the root .
Using this approach, duplicate points are removed and adjusting the resolution of the octree
allows for fine-tuning of the threshold distance between points under which points are considered duplicates.

One advantage of an octree with a fixed width is simplicity, as points are always inserted at the same depth and deletion of points is not required.
The biggest drawback is that the resolution of the point cloud is fixed and needs to be chosen beforehand.
As the accuracy of the depth images differ between devices and other conditions like lighting and texture of the captured surface,
it is difficult to choose a fixed resolution that works for all cases:

When using a resolution that is too high, some surfaces might be detected as multiple surfaces instead of one.
This is due to the same point being detected multiple times with slightly different depth values.
If the uncertainty of the depth data is too high for the chosen resolution,
points representing the same point in space will be added to neighboring cells of the octree.
In the case of planes, this will lead to points in multiple cells across the normal of the plane,
(increasing the thickness of the plane).
The RANSAC algorithm will then detect multiple planes instead of one.

Quantization might also lead to worse detection results for surface that do not align with the axis of the octree.
For example, a plane that is tilted by a couple of degrees will lead to aliasing,
meaning that the distance between the points and the fitted plane will vary across the plane.

\subsubsection{Octree with neighborhood condition}
%todo
To utilize an octree for point cloud storage, the point coordinates and its data is saved in the leaf nodes of the octree.
In this case, each node of the octree saves the coordinate of the point and its confidence value.
When inserting a new point with a certain confidence value, a range query with a radius calculated from the confidence value
is performed to find all nodes within a certain distance of the point to be inserted.
If no node is found, a new leaf node is created and the point is inserted.
If a node is found with a lower confidence value, the new point is inserted and the old node is removed.
If a node is found with a higher confidence value, the new point is not inserted and discarded.
Using this approach, duplicate points are removed and adjusting the multiplier for the radius of the range query
allows for fine-tuning of the threshold under which points are considered duplicates.

\paragraph{Range Query}
To check if an octree node, which is an axis-aligned bounding box (AABB) with equal sides, and a sphere intersect,
the square distance between the center of the sphere and the closest point on the AABB is calculated.
If the square distance is smaller than the square of the radius of the sphere, the sphere and the AABB intersect.

Calculating the square distance between a sphere and the closest point on the AABB can be achieved by summing up the
squared distance in each dimension:
If the sphere's center is outside the extent of the box on a given axis,
the distance is the amount by which it exceeds the box's boundary; otherwise, the distance is zero.
In $n$ dimensions, this can be expressed as
\begin{equation}
    d^2 = \sum_{i=1}^{n} \left\{
    \begin{array}{ll}
    (C_i - B_i - s)
        ^2                & \text{if } C_i > (B_i + s) \\
        (C_i - B_i + s)^2 & \text{if } C_i < (B_i - s) \\
        0                 & \text{otherwise}
    \end{array}
    \right\}
\end{equation}
where $C$ is the center of the sphere, $B$ is the center of the AABB, and $s$ is the half-size of the AABB\@.
\parencite{glassner_graphics_1994}

\paragraph{Deleting nodes}
Deleting nodes from an octree is a non-trivial task~\parencite{samet_design_1989, finkel_quad_1974}, as it may require restructuring the tree to maintain the octree properties.
\citeauthor{finkel_quad_1974}~\parencite{finkel_quad_1974} suggest reinserting all child nodes of the deleted node,
while [] propose a more efficient method, that tries to replace the deleted node with a suited node, such that the octree properties are maintained.
For simplicity, the first approach is used.
In addition, an optimization is made:
As nodes are only deleted when a new point is inserted with a higher confidence value,
it is possible to simply update the old node with the new position, in case the octree properties are not violated.
This is the case when the new point is within the same cell as the old point.
The tree is first traversed once to check if there exists such a node in the search radius.
If such a node is found, the old node is updated and the new point is not inserted.
If such a node is not found, the old node is deleted and the new point is inserted.


\section{Detecting Primitives using RANSAC}

\parencite{schnabel_efficient_2007}


\section{Rendering the primitives}\label{sec:rendering-the-primitives}
The primitives are stored in world space, as in relative to the worlds' origin.
To render the primitives and overlay them onto the camera feed,
transforming the points from world space to screen space is required.

\subsection{OpenGL Rendering Pipeline}
This section provides an overview of the OpenGL rendering pipeline and the necessary steps to render the primitives.
All subsections are based on the book~\citetitle{de_vries_learn_2020} by \citeauthor{de_vries_learn_2020}~\parencite{de_vries_learn_2020}.

\subsubsection{Coordinate Systems}\label{sec:coordinate-systems}
In OpenGL rendering, five coordinate systems are commonly used, as illustrated in figure~\ref{fig:coordinate-systems}.
In these coordinate systems, coordinates are relative to:
\begin{enumerate}
    \item the objects origin (\textit{Local Space})
    \item the worlds origin (\textit{World Space})
    \item the cameras origin (\textit{View Space})
    \item the cameras origin (\textit{Clip Space}) with visible coordinates between -1 and 1 in all dimensions (frustum).
    Coordinates outside the frustum are clipped.
    Perspective projection can be applied here, more details in the next subsection.
    \item the screen (\textit{Screen Space})  with the origin in the bottom left corner of the screen
\end{enumerate}
To transform vertices between these systems, their positions are multiplied with a corresponding transformation matrix.
One exception is the viewport transformation,
which is applied by OpenGL automatically based on the viewport settings provided via the \texttt{glViewport} function.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/coordinate_systems}
    \caption{Coordinate systems}
    \label{fig:coordinate-systems}
\end{figure}

\subsubsection{Perspective Projection}\label{sec:perspective-projection}
In the real world, objects appear smaller the further away they are from the viewer.
To simulate this effect in 3D rendering, perspective projection is used.
Perspective projection is achieved through the use of homogeneous coordinates,
which are an extension of the Cartesian coordinate system.
In homogeneous coordinates, each point in 3D space is represented by four coordinates instead of three: $(x, y, z, w)$.

The transformation from 3D space to a 2D projection is handled by a projection matrix.
This matrix is designed to map the viewable scene, defined within a specified frustum, to a normalized cube known as clip space.
The frustum is a truncated pyramid shape that represents the volume of space visible through the camera lens.
The parameters of the frustum are defined by the field of view, aspect ratio, and near and far clipping planes,
as illustrated by figure~\ref{fig:perspective}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.50\textwidth]{images/perspective}
    \caption{Perspective Projection}
    \label{fig:perspective}
\end{figure}

"The projection matrix [\ldots] also manipulates the w value of each vertex coordinate in such a way
that the further away a vertex coordinate is from the viewer, the higher this w component becomes"~\parencite{de_vries_learn_2020}.
When the coordinates are later divided by $w$ (a process known as the perspective divide), %TODO QUOTAION NEEDED
it results in the desired perspective scaling effect.
Points closer to the viewer have a smaller $w$ and are less affected by the divide,
while points further away have a larger $w$ and are reduced in size more significantly

\subsubsection{Shaders}\label{subsec:shaders}
Shaders are isolated programs that run on the GPU and can be used to render objects.
In OpenGL, they are written in the OpenGL Shading Language (GLSL).

Two types of shaders are required to render an object: Vertex shaders and fragment shaders.

Vertex shaders are executed for each vertex defined in the vertex buffer, which is defined on the CPU and
passed to the GPU by copying the data to the GPU's memory, e.g.\ by using the \texttt{glBufferData} function.
A vertex shader can be used to transform the vertices from model space to screen space using the
model, view, and projection matrices.
To achieve this, the Model-View-Projection matrix (MVP) matrix can be passed as a \textit{uniform} to the vertex shader,
which means it is the same for all vertices.
The MVP matrix is the result of multiplying the model matrix, view matrix, and projection matrix.
As the MVP matrix is the same for all vertices of an object, it can be calculated on the CPU and passed to the GPU as a uniform.
The vertex position is then multiplied by the MVP matrix to transform it into clip space.
The resulting position is then passed to the fragment shader by assigning it to the \texttt{gl\_Position} variable.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.70\textwidth]{images/graphics-pipeline}
    \caption{Graphics Pipeline. Blue boxes represent programmable stages. The Geometry Shader is optional and not covered in this thesis.}
    \label{fig:graphics-pipeline}
\end{figure}


The graphics pipeline then rasterizes and interpolates the vertex positions alongside all other vertex attributes
across the primitive, which are then passed to the fragment shader, which is executed for each pixel the primitive covers.
The fragment shader then outputs the color of the pixel by setting the \texttt{out vec4 FragColor} variable.
Before the final color is written to the framebuffer,
a depth test is performed to determine if the pixel is visible or covered by other pixels from other primitives.

\subsection{Rendering planes}
The RANSAC algorithm provides the parameterization of any detected plane using a normal vector $n$ and the point $p$ relative the worlds origin.
Using OpenGL, a plane can be rendered by creating two triangles composed of 3 vertices each, with two corner vertices shared between the triangles.
To render a plane from the parameterization, one can first find any two vectors $u$ and $v$
that are perpendicular to each-other and to the normal vector $n$.
Using $u$ and $v$, the four vertices of the plane can then be calculated by adding and subtracting $u * size / 2$ and $v * size / 2$ from the point $p$.

\paragraph{Calculating an arbitrary perpendicular vector}
The cross product of two vectors $a$ and $b$ is a vector that is perpendicular to both $a$ and $b$,
as long as $a$ and $b$ are not parallel.
To calculate an arbitrary perpendicular vector to a given vector $n$, one can use any of the 3 basis vectors ${b_1}$, ${b_2}$, and ${b_3}$.
Choosing any of the basis vectors that is not parallel to $n$ will result in a perpendicular vector.
To minimize floating point errors, which are largest for planes where $n$ almost aligns with the chosen basis vector,
the basis vector with the smallest dot product with $n$ can be chosen.
The normalized cross product of two vectors $n$ and $b_{smallest}$ then yields a perpendicular vector to $n$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{images/renderedPlane}
    \caption{Rendered RANSAC plane with size of 2*2m}
\end{figure}

\subsection{Constraining the primitives to the area where the points are located}
To constrain the primitive to the area where the points are located, multiple approaches can be used.
One would be to calculate the maximum and minimum values of the points in each dimension and use these to create a bounding box for the plane.
A more sophisticated approach would be to calculate the convex hull of the points to constrain the plane.

The convex hull of a set of points is the smallest convex polygon that contains all the points. %TODO: Citation needed

To render a plane constrained by the convex hull,
a triangle mesh can be created with triangles consisting of two subsequent triangles of the hull vertices and the centroid of the hull as the third vertex each.
Figure~\ref{fig:convex-hull} shows the result of the convex hull algorithm applied to a real dataset of a plane.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/points}
        \caption{Point data}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/hull}
        \caption{Convex Hull}
    \end{subfigure}%

    \vspace{0.5em}

    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/hull}
        \caption{Rendered Primitive}
    \end{subfigure}%
    \caption{Convex Hull with real data}
    \label{fig:convex-hull}
\end{figure}





\parencite{graham_efficient_1972}
\parencite{andrew_another_1979}

%\subsection{Preparing the data on the CPU}
%
%\subsubsection{Restraining the primitives}
%
%\paragraph{Cross-section => Floor and Walls}
%
%\paragraph{Using a bounding box / extend of points}
%
%\subsubsection{Building a mesh}
%
%\subsection{Rendering on the GPU (shaders)}
