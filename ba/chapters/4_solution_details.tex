\chapter{Solution Details}

Depth information is collected using the ARCore Raw Depth API via depth images and confidence images.
Low confidence points under a certain threshold are filtered out.
Each pixel of the depth image is then projected into three-dimensional world-space, utilizing the camera's intrinsics.
An octree is then use to remove duplicate points as new data arrives.
The RANSAC algorithm is then applied to the point cloud to detect geometric primitives.
%todo


\section{Capturing Depth Images}

\subsection{ARCore Depth APIs}


Google ARCore provides two APIs to access depth information: the Depth API and the Raw Depth API\@.
Both APIs work by estimating depth information from a sequence of monocular camera images using depth-from-motion techniques.
Depth data becomes available when the user moves their device and Google claims that the algorithm
can get accurate results from 0 to 65 meters away, with best results between 0.5 and 5 meters.
The technical details of are further discussed in section~\ref{sec:technical-background-depth-from-motion}.

Both the Raw Depth API and Full Depth API provide depth information for a given frame of a camera image using depth images, but they differ in the level of detail they provide:

The Raw Depth API provides depth images and confidence images, where some pixels may not have any depth information.
The depth image provides the distance from the camera of a given pixel in millimeters.
The confidence image indicates the reliability of the depth information for each pixel, ranging from 0 (no confidence) to 255 (high confidence).

In contrast, the Full Depth API provides a single depth image, where each pixel has a depth value.
To achieve this, values for pixels without depth information are interpolated.
No confidence image is provided.

\begin{figure}[ht!]
    \centering
    % First row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_full-depth-image}
        \caption{Full Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_camera-image}
        \caption{Camera image}
    \end{subfigure}%

    % Optional: Adjust or remove vertical spacing between the rows
    \vspace{0.5em}

    % Second row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-image}
        \caption{Raw Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-confidence-image}
        \caption{Raw Depth API confidence image}
    \end{subfigure}%

    \caption{Full Depth vs. Raw Depth}
    \label{fig:depth-api-images}
\end{figure}


The Depth API is preferred in cases where it is crucial to have a depth value for every pixel, such as calculating if an object should be occluded by the scene in an AR application,
while the Raw depth API is preferred if accuracy of the depth information is crucial.
As accuracy is crucial for primitive detection and depth information for every pixel is not required, the Raw Depth API is used in this project.

\parencite{google_llc_arcore_doc}

\subsection{Technical Background: Depth from Motion}\label{sec:technical-background-depth-from-motion}
Depth from motion is a technique developed by Google that estimates depth information from a sequence of monocular camera images
and is used by the ARCore Depth API to provide depth information.
Its primary purpose is to enable AR applications that rely on depth information on devices lacking a dedicated depth sensor or numerous cameras.
This sections provides a brief overview of the workings of the Depth from Motion system by ~\parencite{valentin_depth_2018}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{images/DepthFromMotion}
    \caption{Overview of the Depth from Motion System}
\end{figure}
When users move their smartphones, the system uses ARCore's visual-inertial odometry (VIO)
to determine its position and orientation in six dimensions (6DoF): up/down, left/right, forward/backward, and tilt/swivel/rotate.
After activating tracking and acquiring the most recent camera image (in black and white for faster processing),
a reference image or keyframe from the past is chosen to compare with the current image.

A process known as polar rectification then aligns the keyframe and current frame onto the same plane based on their differences in position and orientation.
This alignment simplifies the process of finding matching points in both images by focusing on comparable horizontal lines.

Next, they use an image correspondence algorithm to identify matching points,
which generates disparity maps that indicate the positional differences between matched points in the two images.
As scenes may contain low textured objects or repetitive patterns which can lead to incorrect matches,
they use an invalidation step, which removes points that are likely to be incorrect matches.
This step also provides a confidence value, which is later used in the interpolation step.
Triangulation is used to compute a sparse depth map based on the disparity maps.

The missing values in the sparse depth maps are then interpolated using a variation of an algorithm called the bilateral solver,
which yields an intermediate data structure known as a bilateral depth grid.
This grid is a structured representation of depth information that can be converted into a full depth map on demand.
assessable through the Full Depth API\@.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral1}
        \caption{Raw point cloud}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral2}
        \caption{Interpolated}
    \end{subfigure}%
    \caption{Interpolation with Bilateral Solver}
\end{figure}

From the paper it is unclear, which part of the pipeline are omitted in the Raw Depth API,
but it is reasonable to assume that the Raw Depth API accesses the sparse depth map and confidence values before the interpolation step.

\cite{valentin_depth_2018}


\section{Building the point cloud}
As RANSAC is a point-based algorithm, the depth image has to be converted to a point cloud.
The process consists of three steps.
\begin{enumerate}
    \item Filtering low confidence points
    \item Transforming depth image pixels to world coordinate points
    \item Inserting new points into the point cloud
\end{enumerate}

\subsection{Filtering low confidence points}
The Raw Depth API provides a confidence image that indicates the reliability of the depth information for each pixel.
To improve the accuracy of the point cloud, points with low confidence are filtered out.
A threshold value is set, below which points are discarded.
Filtering out low confidence points early in the pipeline will also improve performance,
as fewer points need to be processed in the following steps.

\subsection{Transforming depth image pixels to world coordinate points}
Before inserting points into the point cloud, the depth images values from the depth images
first need to be converted into 3-dimensional coordinates relative to a fixed origin in the world.
This section will first provide a brief overview of the mathematical concepts required to understand the transformation process.
Then, the process of transforming a point from the depth image into world space will be explained.

%To convert the point into world coordinates, first the camera intrinsics are used to
%project the point into camera space.
%Then, the model matrix is used to transform the point into world space.
%\parencite{google_llc_codelab_raw_depth,google_llc_arcore_doc}

\subsubsection{Transformations}

In computer graphics matrices are used to represent geometric transformations like translation, scaling, rotation,
shearing, reflection and projection.
While it is out of scope to cover all of these transformations in detail,
this section will use the example of the translation to explain the concepts of transformations and homogenous coordinates.

To apply transformations to a point, the points is represented in homogenous coordinates,
which is a 4x1 matrix with the fourth $w$ element set to 1.

\begin{equation}
    p = \begin{bmatrix}
            w \cdot x \\
            w \cdot y \\
            w \cdot z \\
            w
    \end{bmatrix}
\end{equation}

The homogenous coordinates are then multiplied with a transformation matrix $M$ to apply the transformation.
\begin{equation}
    p' = M \cdot p
\end{equation}
In the case of a translation, the translation matrix is a 4x4 matrix with the translation vector $t$ as the fourth column.
\begin{equation}
    p' = \begin{bmatrix}
             1 & 0 & 0 & t_x \\
             0 & 1 & 0 & t_y \\
             0 & 0 & 1 & t_z \\
             0 & 0 & 0 & 1
    \end{bmatrix} \cdot \begin{bmatrix}
                            w \cdot x \\
                            w \cdot y \\
                            w \cdot z \\
                            w
    \end{bmatrix} = \begin{bmatrix}
                        w \cdot (x + t_x) \\
                        w \cdot (y + t_y) \\
                        w \cdot (z + t_z) \\
                        w
    \end{bmatrix}
\end{equation}

Note that the fourth element $w$ is not always 1, as later explained in the section about perspective projection.
Thus, after applying the transformation matrix, the resulting matrix is then divided by the fourth element $w$.
The cartesian coordinates of the point are then the first three elements of the resulting matrix.

One advantage of using matrices to represent transformations is that multiple transformations can be combined by
multiplying the transformation matrices.
The resulting matrix will then apply all transformations in the order they were multiplied.
\begin{equation}
    p' = (M_n \cdots M_3 \cdot M_2 \cdot M_1) \cdot p
\end{equation}
If multiple transformations were to be applied to thousands of points,
it would be more efficient to multiply the transformation matrices once and then apply the resulting matrix to all points.
Graphics Processing Unit's (GPU's) also contain hardware implementation of 4x4 matrix operations,
which further increases the performance of matrix operations over other methods.
Transformation matrices can also be inverted ($M^{-1}$) to apply the inverse transformation, or in other words 'undo' the transformation.

\cite{dorner_virtual_2019}

\subsubsection{Coordinate Systems and Basis Change}

Different coordinate systems, also known as spaces, are commonly used to represent points in space.
This simplifies calculations.
For instance, consider a camera inside a moving car, filming a person inside the car.
A smooth camera movement around the person is to be implemented.
If calculations are performed based on coordinates relative to the world (world space),
the movement of the car needs to be accounted for in the calculation for the camera movement in each frame.
However, by using the local coordinate system of the car, where all points are relative to the car itself (local space),
calculations can be simplified.

This section will provide a brief mathematical overview of coordinate systems and how to transform points between them.
As examples, the world and local space will be used, as they are required to unproject the depth image into world space.
The usage of these coordinate systems in context of the OpenGL rendering pipeline will also be covered in section~\ref{sec:coordinate-systems}.

In the context of linear algebra,
a basis of a $n$ dimensional vector space is a set of $n$ vectors that are both linearly independent
and capable representing any geometric vector in the vector space as a linear combination of these basis vectors.
If the basis vectors are orthogonal and normalized,
they are called an orthonormal basis (provided that the inner product is defined on the vector space).
Combined with an origin $O$ it defines a cartesian coordinate system $K$.
Using that coordinate system, any point $P$ in the vector space can be represented with a $n$ dimensional
vector$\vec{p}$ relative to the origin $O$.
In the case of a three-dimensional space and basis vectors $\vec{u}, \vec{v}, \vec{w}$, this is expressed as:
\begin{equation}
    P = O + a\vec{u} + b\vec{v} + c\vec{w} = O + \vec{p}
\end{equation}

Consider a point $P$ in local space that needs to be transformed into world space.
This transformation requires finding a transformation matrix $M$ that transforms points
from local space $K$ to world space $K'$.
The transformation can be achieved using the formula:
\begin{equation}
    p' = M\cdot p
\end{equation}

$K$ and $K'$ are both three-dimensional cartesian coordinate systems, with 3 basis vectors and one origin each.
Let the basis vectors of $K'$ be $\vec{u}, \vec{v}, \vec{w}$ and the origin $O$.
Note that these are described as relative to $K$.
A 4x4 transformation matrix $M'$ that transforms points from $K'$ to $K$ is then constructed as follows:
\begin{equation}
    M' = \begin{bmatrix}
            \vec{u_x} & \vec{v_x} & \vec{w_x} & O_x \\
            \vec{u_y} & \vec{v_y} & \vec{w_y} & O_y \\
            \vec{u_z} & \vec{v_z} & \vec{w_z} & O_z \\
            0         & 0         & 0         & 1
    \end{bmatrix}
\end{equation}

This matrix can then be used to transform points from $K'$ to $K$, by multiplying the point as homogenous coordinates with $M'$.
Consequently, the transformation matrix $M$ that transforms a point $P$ from $K$ to $K'$ is the inverse of $M'$:
\begin{equation}
    p' = M'^{-1} \cdot p = M\cdot p
\end{equation}

A transformation matrix, that transforms points from local space to world space can also be referred as $T_{wl}$:
\begin{equation}
    p_w = T_{wl} \cdot p_l
\end{equation}

\cite{dorner_virtual_2019}

\subsubsection{Common Coordinate Systems}\label{sec:coordinate-systems}
To further clarify the concept of coordinate systems, this section provides an overview of the most common coordinate systems,
specifically in the context of the OpenGL rendering pipeline, which will later be used to render the primitives.
In OpenGL rendering, five coordinate systems are commonly used, as illustrated in figure~\ref{fig:coordinate-systems}.
In these coordinate systems, coordinates are relative to:
\begin{enumerate}
    \item the objects origin (\textit{Local Space})
    \item the worlds origin (\textit{World Space})
    \item the cameras origin (\textit{View Space})
    \item the cameras origin (\textit{Clip Space}) with visible coordinates between -1 and 1 (normalized device coordinates) in all dimensions (frustum).
    Coordinates outside the frustum are clipped.
    Perspective projection can be applied here, more details in the next subsection.
    \item the screen (\textit{Screen Space})  with the origin in the bottom left corner of the screen
\end{enumerate}
To transform points between these systems, their positions are multiplied with a corresponding transformation matrix.
One exception is the viewport transformation,
which is applied by OpenGL automatically based on the viewport settings provided via the \texttt{glViewport} function.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/coordinate_systems}
    \caption{Coordinate systems}
    \label{fig:coordinate-systems}
\end{figure}

\cite{de_vries_learn_2020}


\subsubsection{Camera Intrinsics and Perspective Projection}
In the real world, objects appear smaller the further away they are from the viewer.
The same concept applies to images captured by cameras.

\begin{figure}[h!]
    \centering
\end{figure}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth} % Adjusted from 0.4 to 0.45
        \includegraphics[width=1\linewidth]{images/intrinsics}
        %    source: szeliski_computer_nodate
        \caption{Overview of intrinsic parameters in 3D}
    \end{subfigure}%
    \hspace{0.05\textwidth} % Added space between subfigures
    \begin{subfigure}[t]{0.45\textwidth} % Adjusted from 0.4 to 0.45
        \resizebox{\linewidth}{!}{
            \input{images/intrinsics2d}
        }
    \caption{Point $A$ projected onto the image plane in $y$-direction}
    \end{subfigure}%
    \caption{Camera intrinsics}
    \label{fig:intrinsics}
\end{figure}


\cite{szeliski_computer_nodate}

\subsubsection{Applying the transformation to the depth pixels}
"Given point $A$ on the observed real-world geometry and a 2D point $a$ representing the same point in the depth image,
the value given by the Depth API at $a$ is equal to the length of $CA$ projected onto the principal axis.
This can also be referred as the z-coordinate of $A$ relative to the camera origin $C$."~\parencite{google_llc_arcore_doc}
%See figure~\ref{fig:diagram-depth-values}.
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.75\textwidth]{images/depth-values-diagram}
%    \caption{Diagram of depth values from the Depth API}
%    \label{fig:diagram-depth-values}
%\end{figure}
This can be seen as the pixels being in screen space, as shown in figure~\ref{fig:coordinate-systems}.
To convert these points into world space, the transformations need to be applied backwards:
\begin{enumerate}
    \item From screen space to view space (`unprojecting`)
    \item From view space to world space
\end{enumerate}
%`Unprojecting` the pixel from screen space to view space and then to world space is required.
%Note that clip space is not applicable here, as ARCore provides intrinsics in pixels, not normalized device coordinates,
%thus the projection matrix directly transforms from view space to screen space.
Another way to think about this procedure is to conceptualize the unprojecting step as transforming the pixels into a local space
relative to the camera, as defined by the camera's position, up and look vector.
Subsequently, the point is transformed from the cameras local space into world space using the cameras model matrix.
For simplicity and to avoid confusion when transforming into different directions, the second approach is used going forward.


To transform the pixel, first the depth value is used to calculate the point in local space relative to the camera $K_l$ using the camera intrinsics.
The intrinsics can be retrieved from the API using \texttt{frame.getCamera().getTextureIntrinsics()} method.
However, this method returns the intrinsics of the camera image, which differs from the depth image, as the depth image usually has a lower resolution.
To calculate the focal length $f$ and camera center $c$ from the provided intrinsics $I$ and the depth image $D$,
the provided focal length and principal point are scaled by the ratio of the dimensions of the depth image and the camera image:
\begin{equation}
    S = \begin{bmatrix}
            \frac{\dim_x(D)}{I_{xDim}} & 0\\[8pt]
            0                          & \frac{\dim_y(D)}{I_{yDim}}
    \end{bmatrix}\\
    f = S \cdot I_f \\
    c = S \cdot I_c
\end{equation}

These values are then used to unproject the point into local space relative to the camera:
\begin{equation}
    p_l = \begin{pmatrix}
              d \cdot (x - c_x) / f_x \\
              d \cdot (c_y - y) / f_y \\
              -d                      \\
              1
    \end{pmatrix}
\end{equation}

As the resulting point is in a local space relative to the camera $K_l$, it needs to be transformed into world space $K_w$.
To transform it into world space the model matrix is retrieved by calling
\texttt{cameraPoseAnchor.getPose().toMatrix(modelMatrix, 0)}.
%The model matrix is a transformation matrix that transforms points from a local space to world space.
By multiplying the model matrix $T_{wl}$ with $p_l$, the point is transformed into world space.
\begin{equation}
    p_w = T_{wl} \cdot p_l
\end{equation}

\cite{google_llc_codelab_raw_depth, google_llc_arcore_doc}

\subsection{Inserting new points into the point cloud}

The final step involves adding new points to the point cloud.
However, simply storing a list of all points and appending new data to it is not feasible.
As a new depth image is captured every frame, the number of points would grow endlessly.
This approach would also result in a massive number of points representing the same real-world point,
but with slightly different position values,
as subsequent depth images will show the same real-world point from different angles, with depth values varying slightly.
To address this issue, a spatial data structure that partitions the space into smaller regions and
allows us to store only one point per region can be used.
One possible approach is to use a three-dimensional grid, where each cell represents a region in space.
Alternatively, a more advanced approach involves utilizing a spatial data structure such as an octree,
which can provide improved performance.

\paragraph{Octree}
An octree is a spatial tree data structure where each node has exactly eight children.
The tree can be used to sparsely partition three-dimensional space into smaller cubes and allows for efficient
insertion and traversal in logarithmic time.
"For the definition a simple recursive splitting of [cubes] is continued until there is only one point in a [cube]."
\parencite{gabriel_zachmann_geometric_2002}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{images/octree}
    \caption{Octree}
    \label{fig:octrree}
\end{figure}

\subsubsection{Fixed Depth Octree}
A straightforward approach to utilize an octree for point cloud storage is to use a variation of the octree with a fixed depth.
Points are always inserted at the defined depth of the tree, creating all nodes up to that depth if they do not yet exist.
The center coordinate of the leaf node can then be inferred as the point in space,
thus saving the coordinates of the point explicitly is not required.
Instead, to find the coordinates of a given node, the tree needs to be traversed while keeping track of
the extent and center coordinate of the current node.
This approach will naturally provide quantization of the point cloud, with the resolution of the point cloud
determined by the depth of the octree and extend of the root.
This is functionally equal to a three-dimensional grid,
but with the advantage of logarithmic time complexity for insertion and traversal.
Using this approach, duplicate points are removed and adjusting the resolution of the octree
allows for fine-tuning of the threshold distance between points under which points are considered duplicates.

One advantage of an octree with a fixed width is simplicity, as points are always inserted at the same depth and deletion of points is not required.
The biggest drawback is that the resolution of the point cloud is fixed and needs to be chosen beforehand.
As the accuracy of the depth images differ between devices and other conditions like lighting and texture of the captured surface,
it is difficult to choose a fixed resolution that works for all cases:

When using a resolution that is too high, some surfaces might be detected as multiple surfaces instead of one.
This is due to the same point being detected multiple times with slightly different depth values.
If the uncertainty of the depth data is too high for the chosen resolution,
points representing the same point in space will be added to neighboring cells of the octree.
In the case of planes, this will lead to points in multiple cells across the normal of the plane,
(increasing the thickness of the plane).
The RANSAC algorithm will then detect multiple planes instead of one.

Quantization might also lead to worse detection results for surface that do not align with the axis of the octree.
For example, a plane that is tilted by a couple of degrees will lead to aliasing,
meaning that the distance between the points and the fitted plane will vary across the plane.

\subsubsection{Octree with neighborhood condition}
%todo
To utilize an octree for point cloud storage, the point coordinates and its data is saved in the leaf nodes of the octree.
In this case, each node of the octree saves the coordinate of the point and its confidence value.
When inserting a new point with a certain confidence value, a range query with a radius calculated from the confidence value
is performed to find all nodes within a certain distance of the point to be inserted.
If no node is found, a new leaf node is created and the point is inserted.
If a node is found with a lower confidence value, the new point is inserted and the old node is removed.
If a node is found with a higher confidence value, the new point is not inserted and discarded.
Using this approach, duplicate points are removed and adjusting the multiplier for the radius of the range query
allows for fine-tuning of the threshold under which points are considered duplicates.

\paragraph{Range Query}
To check if an octree node, which is an axis-aligned bounding box (AABB) with equal sides, and a sphere intersect,
the square distance between the center of the sphere and the closest point on the AABB is calculated.
If the square distance is smaller than the square of the radius of the sphere, the sphere and the AABB intersect.

Calculating the square distance between a sphere and the closest point on the AABB can be achieved by summing up the
squared distance in each dimension:
If the sphere's center is outside the extent of the box on a given axis,
the distance is the amount by which it exceeds the box's boundary; otherwise, the distance is zero.
In $n$ dimensions, this can be expressed as
\begin{equation}
    d^2 = \sum_{i=1}^{n} \left\{
    \begin{array}{ll}
    (C_i - B_i - s)
        ^2                & \text{if } C_i > (B_i + s) \\
        (C_i - B_i + s)^2 & \text{if } C_i < (B_i - s) \\
        0                 & \text{otherwise}
    \end{array}
    \right\}
\end{equation}
where $C$ is the center of the sphere, $B$ is the center of the AABB, and $s$ is the half-size of the AABB\@.
\parencite{glassner_graphics_1994}

\paragraph{Deleting nodes}
Deleting nodes from an octree is a non-trivial task~\parencite{samet_design_1989, finkel_quad_1974}, as it may require restructuring the tree to maintain the octree properties.
\citeauthor{finkel_quad_1974}~\parencite{finkel_quad_1974} suggest reinserting all child nodes of the deleted node,
while [] propose a more efficient method, that tries to replace the deleted node with a suited node, such that the octree properties are maintained.
For simplicity, the first approach is used.
In addition, an optimization is made:
As nodes are only deleted when a new point is inserted with a higher confidence value,
it is possible to simply update the old node with the new position, in case the octree properties are not violated.
This is the case when the new point is within the same cell as the old point.
The tree is first traversed once to check if there exists such a node in the search radius.
If such a node is found, the old node is updated and the new point is not inserted.
If such a node is not found, the old node is deleted as by~\cite{finkel_quad_1974} and the new point is inserted.


\section{Detecting Primitives using RANSAC}

\parencite{schnabel_efficient_2007}


\section{Rendering the primitives}\label{sec:rendering-the-primitives}
The primitives are stored in world space, as in relative to the worlds' origin.
To render the primitives and overlay them onto the camera feed,
transforming the points from world space to screen space is required.

\subsection{OpenGL Rendering Pipeline}
This section provides an overview of the OpenGL rendering pipeline and the necessary steps to render the primitives.
All subsections are based on the book~\citetitle{de_vries_learn_2020} by \citeauthor{de_vries_learn_2020}~\parencite{de_vries_learn_2020}.


\subsubsection{Perspective Projection}\label{sec:perspective-projection}
In the real world, objects appear smaller the further away they are from the viewer.
To simulate this effect in 3D rendering, perspective projection is used.
Perspective projection is achieved through the use of homogeneous coordinates,
which are an extension of the Cartesian coordinate system.
In homogeneous coordinates, each point in 3D space is represented by four coordinates instead of three: $(x, y, z, w)$.

The transformation from 3D space to a 2D projection is handled by a projection matrix.
This matrix is designed to map the viewable scene, defined within a specified frustum, to a normalized cube known as clip space.
The frustum is a truncated pyramid shape that represents the volume of space visible through the camera lens.
The parameters of the frustum are defined by the field of view, aspect ratio, and near and far clipping planes,
as illustrated by figure~\ref{fig:perspective}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.50\textwidth]{images/perspective}
    \caption{Perspective Projection}
    \label{fig:perspective}
\end{figure}

"The projection matrix [\ldots] also manipulates the w value of each vertex coordinate in such a way
that the further away a vertex coordinate is from the viewer, the higher this w component becomes"~\parencite{de_vries_learn_2020}.
When the coordinates are later divided by $w$ (a process known as the perspective divide), %TODO QUOTAION NEEDED
it results in the desired perspective scaling effect.
Points closer to the viewer have a smaller $w$ and are less affected by the divide,
while points further away have a larger $w$ and are reduced in size more significantly

\subsubsection{Shaders}\label{subsec:shaders}
Shaders are isolated programs that run on the GPU and can be used to render objects.
In OpenGL, they are written in the OpenGL Shading Language (GLSL).

Two types of shaders are required to render an object: Vertex shaders and fragment shaders.

Vertex shaders are executed for each vertex defined in the vertex buffer, which is defined on the CPU and
passed to the GPU by copying the data to the GPU's memory, e.g.\ by using the \texttt{glBufferData} function.
A vertex shader can be used to transform the vertices from model space to screen space using the
model, view, and projection matrices.
To achieve this, the Model-View-Projection matrix (MVP) matrix can be passed as a \textit{uniform} to the vertex shader,
which means it is the same for all vertices.
The MVP matrix is the result of multiplying the model matrix, view matrix, and projection matrix.
As the MVP matrix is the same for all vertices of an object, it can be calculated on the CPU and passed to the GPU as a uniform.
The vertex position is then multiplied by the MVP matrix to transform it into clip space.
The resulting position is then passed to the fragment shader by assigning it to the \texttt{gl\_Position} variable.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.70\textwidth]{images/graphics-pipeline}
    \caption{Graphics Pipeline. Blue boxes represent programmable stages. The Geometry Shader is optional and not covered in this thesis.}
    \label{fig:graphics-pipeline}
\end{figure}


The graphics pipeline then rasterizes and interpolates the vertex positions alongside all other vertex attributes
across the primitive, which are then passed to the fragment shader, which is executed for each pixel the primitive covers.
The fragment shader then outputs the color of the pixel by setting the \texttt{out vec4 FragColor} variable.
Before the final color is written to the framebuffer,
a depth test is performed to determine if the pixel is visible or covered by other pixels from other primitives.

\subsubsection{Triangle Meshes}
todo

\subsection{Rendering planes}
The RANSAC algorithm provides the parameterization of any detected plane using a normal vector $n$ and the point $p$ relative the worlds origin.
Using OpenGL, a plane can be rendered by creating two triangles composed of 3 vertices each, with two corner vertices shared between the triangles.
To render a plane from the parameterization, one can first find two arbitrary vectors $u$ and $v$
that are perpendicular to each-other and to the normal vector $n$.
Using $u$ and $v$, the four corner vertices of the plane can then be calculated by adding and subtracting $u * size / 2$ and $v * size / 2$ from the point $p$.

\paragraph{Calculating an arbitrary perpendicular vector}
The cross product of two vectors $a$ and $b$ is a vector that is perpendicular to both $a$ and $b$,
as long as $a$ and $b$ are not parallel.
To calculate an arbitrary perpendicular vector to a given vector $n$, one can use any of the 3 basis vectors ${b_1}$, ${b_2}$, and ${b_3}$.
Choosing any of the basis vectors that is not parallel to $n$ will result in a perpendicular vector.
To minimize floating point errors, which are largest for planes where $n$ almost aligns with the chosen basis vector,
the basis vector with the smallest dot product with $n$ can be chosen.
The normalized cross product of two vectors $n$ and $b_{smallest}$ then yields a perpendicular vector to $n$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\linewidth]{images/renderedPlane}
    \caption{Rendered RANSAC plane with size of 2*2m}
\end{figure}

\subsection{Constraining the primitives to the area where the points are located}
To constrain the primitive to the area where the points are located, multiple approaches can be used.
One would be to calculate the maximum and minimum values of the points in each dimension and use these to create a bounding box for the plane.
A more sophisticated approach would be to calculate the convex hull of the points to constrain the plane.

The convex hull of a set of points is the smallest convex polygon that contains all the points. %TODO: Citation needed

To render a plane constrained by the convex hull,
a triangle mesh can be created with triangles consisting of two subsequent triangles of the hull vertices and the centroid of the hull as the third vertex each.
Figure~\ref{fig:convex-hull} shows the result of the convex hull algorithm applied to a real dataset of a plane.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/points}
        \caption{Point data}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/hull}
        \caption{Convex Hull}
    \end{subfigure}%

    \vspace{0.5em}

    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/hull}
        \caption{Rendered Primitive}
    \end{subfigure}%
    \caption{Convex Hull with real data}
    \label{fig:convex-hull}
\end{figure}





\parencite{graham_efficient_1972, andrew_another_1979}

