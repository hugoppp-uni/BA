\chapter{Solution Details}

Depth information is collected using the ARCore Raw Depth API via depth images and confidence images.
Low confidence points under a certain threshold are filtered out.
Each pixel of the depth image is then projected into three-dimensional world-space, utilizing the camera's intrinsics.
An octree is then use to remove duplicate points as new data arrives.
The RANSAC algorithm is then applied to the point cloud to detect geometric primitives.
%todo


\section{Capturing Depth Images}

\subsection{ARCore Depth APIs}

Google ARCore~\parencite{google_llc_arcore_doc} is an SDK for developing augmented reality applications on Android and iOS devices.
It provides a wide range of features, such as motion tracking, environmental understanding, and light estimation.
For the purpose of this project, it is used to access depth information from the camera in form of depth images.

Google ARCore provides two APIs to access depth information -- the Depth API and the Raw Depth API\@.
Both APIs work by estimating depth information from a sequence of monocular camera images using
a technique called \texttt{depth-from-motion}.
When the user moves their device, the system takes a sequence of images and estimates the depth information from the differences between these images.
The technical details of this system are further discussed in section~\ref{sec:technical-background-depth-from-motion}.
Google claims that that one "can get accurate results from 0 to 65 meters away,
with best results between 0.5 and 5 meters."~\parencite{google_llc_arcore_doc}

Both the Raw Depth API and Full Depth API provide depth information for a given frame of a camera image using depth images, but they differ in the level of detail they provide:

The Raw Depth API provides depth images and confidence images, where some pixels may not have any depth information.
The depth image provides the distance from the camera of a given pixel in millimeters.
The confidence image indicates the reliability of the depth information for each pixel, ranging from 0 (no confidence) to 255 (high confidence).

In contrast, the Full Depth API provides a single depth image, where each pixel has a depth value.
To achieve this, values for pixels without depth information are interpolated.
No confidence image is provided.
\begin{figure}[ht!]
    \centering
    % First row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_full-depth-image}
        \caption{Full Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_camera-image}
        \caption{Camera image}
    \end{subfigure}%

    % Optional: Adjust or remove vertical spacing between the rows
    \vspace{0.5em}

    % Second row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-image}
        \caption{Raw Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-confidence-image}
        \caption{Raw Depth API confidence image}
    \end{subfigure}%

    \caption{Full Depth vs. Raw Depth}
    \label{fig:depth-api-images}
\end{figure}
APIs have their use cases --
the Depth API is preferred in cases where it is crucial to have a depth value for every pixel, such as calculating if an object should be occluded by the scene in an AR application,
while the Raw depth API is preferred if accuracy of the depth information is crucial.
As accuracy is crucial for primitive detection and depth information for every pixel is not required, the Raw Depth API is used in this project.

\subsection{Technical Background: Depth from Motion}\label{sec:technical-background-depth-from-motion}
Depth from motion is a technique developed by Google that estimates depth information from a sequence of monocular camera images
and is used by the ARCore Depth API to provide depth information.
Its primary purpose is to enable AR applications that rely on depth information on devices lacking a dedicated depth sensor or numerous cameras.
This sections provides a brief overview of the workings of the Depth from Motion system by ~\parencite{valentin_depth_2018}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{images/DepthFromMotion}
    \caption{Overview of the Depth from Motion System}
\end{figure}
When users move their smartphones, the system uses ARCore's visual-inertial odometry (VIO)
to determine its position and orientation in six dimensions (6DoF): up/down, left/right, forward/backward, and tilt/swivel/rotate.
After activating tracking and acquiring the most recent camera image (in black and white for faster processing),
a reference image or keyframe from the past is chosen to compare with the current image.

A process known as polar rectification then aligns the keyframe and current frame onto the same plane based on their differences in position and orientation.
This alignment simplifies the process of finding matching points in both images by focusing on comparable horizontal lines.

Next, they use an image correspondence algorithm to identify matching points,
which generates disparity maps that indicate the positional differences between matched points in the two images.
As scenes may contain low textured objects or repetitive patterns which can lead to incorrect matches,
they use an invalidation step, which removes points that are likely to be incorrect matches.
This step also provides a confidence value, which is later used in the interpolation step.
Triangulation is used to compute a sparse depth map based on the disparity maps.

The missing values in the sparse depth maps are then interpolated using a variation of an algorithm called the bilateral solver (see figure~\ref{fig:depth-from-motion-interpolation}),
which yields an intermediate data structure known as a bilateral depth grid.
This grid is a structured representation of depth information that can be converted into a full depth map on demand,
assessable through the Full Depth API\@.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral1}
        \caption{Raw point cloud}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DepthFromMotionBilateral2}
        \caption{Interpolated}
    \end{subfigure}%
    \caption{Interpolation with Bilateral Solver}
    \label{fig:depth-from-motion-interpolation}
\end{figure}

From the paper it is unclear, which part of the pipeline are omitted in the Raw Depth API,
but it is reasonable to assume that the Raw Depth API accesses the sparse depth map and confidence values before the interpolation step.


\section{Building the point cloud}
With the depth image and confidence image collected from the Raw Depth API,
the next step is to convert the depth image into a point cloud, as RANSAC is a point-based algorithm.
The process consists of three steps:
\begin{enumerate}
    \item Filtering low confidence points
    \item Transforming depth image pixels to world coordinate points
    \item Inserting new points into the point cloud
\end{enumerate}

\subsection{Filtering low confidence points}
The Raw Depth API provides a confidence image that indicates the reliability of the depth information for each pixel.
To improve the accuracy of the point cloud, points with low confidence are filtered out.
A threshold value is set, below which points are discarded.
Filtering out low confidence points early in the pipeline will also improve performance,
as fewer points need to be processed in the following steps.

\subsection{Transforming depth image pixels to world coordinate points}
The Depth API provides depth images where each pixel holds the distance
from the camera in millimeters.
Before inserting points into the point cloud, the pixel values of the depth image
first need to be converted into 3-dimensional coordinates relative to a fixed origin in the world.
This section will first provide a brief overview of the mathematical concepts required to understand the transformation process.
Then, the process of transforming a point from the depth image into world space will be explained.

%To convert the point into world coordinates, first the camera intrinsics are used to
%project the point into camera space.
%Then, the model matrix is used to transform the point into world space.
%\parencite{google_llc_codelab_raw_depth,google_llc_arcore_doc}

\subsubsection{Transformations}

In computer graphics matrices are used to represent geometric transformations like translation, scaling, rotation,
shearing, reflection and projection.
While it is out of scope to cover all of these transformations in detail,
this section will use the example of the translation to explain the concepts of
transformations and homogenous coordinates based on~\cite{dorner_virtual_2019}.


%TODO Homogenous coordinates explaition
%which are an extension of the Cartesian coordinate system.
%In homogeneous coordinates, each point in 3D space is represented by four coordinates instead of three: $(x, y, z, w)$.

To apply transformations to a point, the points is represented in homogenous coordinates,
which is a 4x1 matrix with the fourth $w$ element set to 1.

\begin{equation}
    p = \begin{bmatrix}
            w \cdot x \\
            w \cdot y \\
            w \cdot z \\
            w
    \end{bmatrix}
\end{equation}

The homogenous coordinates are then multiplied with a transformation matrix $M$ to apply the transformation.
\begin{equation}
    p' = M \cdot p
\end{equation}
In the case of a translation, the translation matrix is a 4x4 matrix with the translation vector $t$ as the fourth column.
\begin{equation}
    p' = \begin{bmatrix}
             1 & 0 & 0 & t_x \\
             0 & 1 & 0 & t_y \\
             0 & 0 & 1 & t_z \\
             0 & 0 & 0 & 1
    \end{bmatrix} \cdot \begin{bmatrix}
                            w \cdot x \\
                            w \cdot y \\
                            w \cdot z \\
                            w
    \end{bmatrix} = \begin{bmatrix}
                        w \cdot (x + t_x) \\
                        w \cdot (y + t_y) \\
                        w \cdot (z + t_z) \\
                        w
    \end{bmatrix}
\end{equation}

Note that the fourth element $w$ is not always 1, as later explained in the section about perspective projection.
Thus, after applying the transformation matrix, the resulting matrix is then divided by the fourth element $w$.
The cartesian coordinates of the point are then the first three elements of the resulting matrix.

One advantage of using matrices to represent transformations is that multiple transformations can be combined by
multiplying the transformation matrices.
The resulting matrix will then apply all transformations in the order they were multiplied.
\begin{equation}
    p' = (M_n \cdots M_3 \cdot M_2 \cdot M_1) \cdot p
\end{equation}
If multiple transformations were to be applied to thousands of points,
it would be more efficient to multiply the transformation matrices once and then apply the resulting matrix to all points.
Graphics Processing Unit's (GPU's) also contain hardware implementation of 4x4 matrix operations,
which further increases the performance of matrix operations over other methods.
Transformation matrices can also be inverted ($M^{-1}$) to apply the inverse transformation, or in other words 'undo' the transformation.

\subsubsection{Coordinate Systems and Basis Change}

Different coordinate systems, also known as spaces, are commonly used to represent points in space.
This simplifies calculations.
For instance, consider a camera inside a moving car, filming a person inside the car.
A smooth camera movement around the person is to be implemented.
If calculations are performed based on coordinates relative to the world (world space),
the movement of the car needs to be accounted for in the calculation for the camera movement in each frame.
However, by using the local coordinate system of the car, where all points are relative to the car itself (local space),
calculations can be simplified.

This section provides an overview of coordinate systems and the process of transforming points between systems, as detailed by~\cite{dorner_virtual_2019}.
%As examples, the world and local space are used, as they are required to unproject the depth image into world space.
%The usage of these coordinate systems in context of the OpenGL rendering pipeline will also be covered in section~\ref{sec:coordinate-systems}.

In the context of linear algebra,
a basis of a $n$ dimensional vector space is a set of $n$ vectors that are both linearly independent
and capable representing any geometric vector in the vector space as a linear combination of these basis vectors.
If the basis vectors are orthogonal and normalized,
they are called an orthonormal basis (provided that the inner product is defined on the vector space).
Combined with an origin $O$ it defines a cartesian coordinate system $K$.
Using that coordinate system, any point $P$ in the vector space can be represented with a $n$ dimensional
vector$\vec{p}$ relative to the origin $O$.
In the case of a three-dimensional space and basis vectors $\vec{u}, \vec{v}, \vec{w}$, this is expressed as:
\begin{equation}
    P = O + a\vec{u} + b\vec{v} + c\vec{w} = O + \vec{p}
\end{equation}

Consider a point $P$ in local space that needs to be transformed into world space.
This transformation requires finding a transformation matrix $M$ that transforms points
from local space $K$ to world space $K'$.
The transformation can be achieved using the formula:
\begin{equation}
    p' = M\cdot p
\end{equation}

$K$ and $K'$ are both three-dimensional cartesian coordinate systems, with 3 basis vectors and one origin each.
Let the basis vectors of $K'$ be $\vec{u}, \vec{v}, \vec{w}$ and the origin $O$.
Note that these are described as relative to $K$.
A 4x4 transformation matrix $M'$ that transforms points from $K'$ to $K$ is then constructed as follows:
\begin{equation}
    M' = \begin{bmatrix}
             \vec{u_x} & \vec{v_x} & \vec{w_x} & O_x \\
             \vec{u_y} & \vec{v_y} & \vec{w_y} & O_y \\
             \vec{u_z} & \vec{v_z} & \vec{w_z} & O_z \\
             0         & 0         & 0         & 1
    \end{bmatrix}
\end{equation}

This matrix can then be used to transform points from $K'$ to $K$, by multiplying the point as homogenous coordinates with $M'$.
Consequently, the transformation matrix $M$ that transforms a point $P$ from $K$ to $K'$ is the inverse of $M'$:
\begin{equation}
    p' = M'^{-1} \cdot p = M\cdot p
\end{equation}

A transformation matrix, that transforms points from local space to world space can also be referred as $T_{wl}$:
\begin{equation}
    p_w = T_{wl} \cdot p_l
\end{equation}

\paragraph{Common Coordinate Systems}\label{sec:coordinate-systems}
The following coordinate systems are commonly used in computer graphics, as illustrated in figure~\ref{fig:coordinate-systems}.
In these coordinate systems, coordinates are relative to:
\begin{enumerate}
    \item the objects origin (\textit{Local Space})
    \item the worlds origin (\textit{World Space})
    \item the cameras origin (\textit{View Space})
    \item the bottom left corner of the screen in 2D (\textit{Screen Space})
\end{enumerate}
Note that in rendering, an intermediate coordinate system between view and screen space is often used (clip space),
which is omitted here for clarity.

To transform points between these systems, their positions are multiplied with a corresponding transformation matrix.
To improve performance, these matrices are often combined to a singular matrix, called the MVP-Matrix,
that transforms points from local space to screen space. \cite{de_vries_learn_2020}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{images/coordinateSystemsAdjusted}
    \caption{Coordinate systems}
    \label{fig:coordinate-systems}
\end{figure}

\subsubsection{Perspective Projection and Camera Intrinsics}\label{sec:perspective-projection}
In the real world, objects appear smaller the further away they are from the viewer.
The same concept applies to images captured by cameras.
To illustrate this, consider a simpel pinhole camera model, as shown in figure~\ref{fig:pinhole-camera-model}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\linewidth]{images/pinhole}
    \caption{Pinhole camera model}
    \label{fig:pinhole-camera-model}
\end{figure}
The camera is represented as a box with a small hole on one side.
Light rays from the scene pass through the hole and project an inverted image onto the opposite side of the box.
The red lines each represent the light ray of the top and bottom of the tree.
The closer the tree is to the camera, the smaller it needs to be to fit inside the two rays,
or in other words to appear the same size on the image plane.
This is the principle of perspective projection and can be achieved by dividing the
$x$ and $y$ coordinates of a point by its $z$ coordinate.
During this process, the z coordinate is lost and can't be recovered. \cite{szeliski_computer_nodate}
\begin{equation}
    P_s = \begin{bmatrix}
              x/z \\
              y/z \\
              1
    \end{bmatrix}
\end{equation}

This however, does not account for the geometry of the camera itself -- the camera intrinsics.
These intrinsics are a set of parameters that describe the camera's geometry and are used to calculate the projection.
The two most important parameters are the focal length $f$ and the image center $c$, as shown in figure~\ref{fig:intrinsics-3d}.


\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth} % Adjusted from 0.4 to 0.45
        \includegraphics[width=1\linewidth]{images/intrinsics}
        %    source: szeliski_computer_nodate
        \caption{Geometry of a camera with an image plane height $H$, width $W$, focal length $f$, and principal point $c$.}
        \label{fig:intrinsics-3d}
    \end{subfigure}%
    \hspace{0.05\textwidth} % Added space between subfigures
    \begin{subfigure}[t]{0.45\textwidth} % Adjusted from 0.4 to 0.45
        \resizebox{\linewidth}{!}{
            \input{images/intrinsics2d}
        }
        \caption{Projection of an observed point $A$ onto the image plane in the $y$-direction.}
        \label{fig:intrinsics-2d}
    \end{subfigure}%
    \caption{Camera intrinsics}
    \label{fig:intrinsics}
\end{figure}

The focal length $f$ is the distance between the image plane and the pinhole of the camera.
The image center $c$ (also called principal point) is the point where the principal axis $z_c$
intersects the image plane, denoted in pixel coordinates.
Figure~\ref{fig:intrinsics-2d} shows the projection of an observed point $A$ onto the image plane.
Note that the image plane is displayed in the $-z_c$ direction, as opposed to $+z_c$ direction.
It shows how focal length $f$ affects the $y$-coordinate of the projected point.
\cite{szeliski_computer_nodate, google_llc_arcore_doc, kris_kitani_computer_2017}
This can be simulated by using ones fingers to form a square that represents a virtual
image plane and moving it closer or farther to the eye.
The farther away the square is (higher focal length), the less field of view the square covers.

Mathematically, the projection is often represented as a matrix, called the calibration matrix
\begin{equation}
    K = \begin{bmatrix}
            f_x & 0   & c_x \\
            0   & f_y & c_y \\
            0   & 0   & 1
    \end{bmatrix}
\end{equation}
with independent focal lengths $f_x$ and $f_y$ to account for aspect ratios other than 1:1.
As different focal lengths in each dimension do not reflect the real geometry of a camera,
a more intuitive way to understand different focal lengths can be used by introducing the aspect ratio a:
\begin{equation}
    K = \begin{bmatrix}
            f & 0  & c_x \\
            0 & af & c_y \\
            0 & 0  & 1
    \end{bmatrix}
\end{equation}
\parencite{szeliski_computer_nodate}




\subsubsection{Applying the transformation to the depth pixels}
With the mathematical concepts clarified, the transformation of depth image pixels into world space can be explained.
First, it is necessary to understand how the depth values are represented in the depth image.

In terms of coordinate systems, the depth values are in screen space, as shown in figure~\ref{fig:coordinate-systems}.
To convert these points into world space, the transformations need to be applied backwards:
\begin{enumerate}
    \item From screen space to view space (`unprojecting`)
    \item From view space to world space
\end{enumerate}
%`Unprojecting` the pixel from screen space to view space and then to world space is required.
%Note that clip space is not applicable here, as ARCore provides intrinsics in pixels, not normalized device coordinates,
%thus the projection matrix directly transforms from view space to screen space.
Another way to think about this procedure is to conceptualize the unprojecting step as transforming the pixels into a local space
relative to the camera, as defined by the camera's position, up and look vector.
Subsequently, the point is transformed from the cameras local space into world space using the cameras model matrix.
For simplicity and to avoid confusion when transforming into different directions, the second approach is used going forward.

The first step is to unproject the pixels from screen space to the cameras local space $K_l$, as explained by~\cite{google_llc_codelab_raw_depth, google_llc_arcore_doc}.
Referring to figure~\ref{fig:intrinsics} and
"given point $A$ on the observed real-world geometry and a 2D point $a$ representing the same point in the depth image,
the value given by the Depth API at $a$ is equal to the length of $CA$ projected onto the principal axis.
This can also be referred as the z-coordinate of $A$ relative to the camera origin $C$."~\parencite{google_llc_arcore_doc}

The depth value is used to calculate the point in local space relative to the camera $K_l$ using the camera intrinsics.
The intrinsics can be retrieved from the API using the method \texttt{frame.getCamera().getTextureIntrinsics()}.
However, this method returns the intrinsics of the camera image, which differs from the depth image, as the depth image usually has a lower resolution.
To calculate the focal length $f$ and camera center $c$ from the provided intrinsics $K$, depth image $D$ and camera image dimensions $x_{Dim}, y_{Dim}$,
the provided focal length and principal point are scaled by the ratio of the dimensions of the depth image and the camera image:
\begin{equation}
    S = \begin{bmatrix}
            \frac{\dim_x(D)}{x_{Dim}} & 0\\[8pt]
            0                         & \frac{\dim_y(D)}{y_{Dim}}
    \end{bmatrix}\\
    f = S \cdot K_f \\
    c = S \cdot K_c
\end{equation}

These values are then used to unproject the point into local space relative to the camera $K_l$ using the equation
\begin{equation}
    p_l = \begin{pmatrix}
              d \cdot (x - c_x) / f_x \\
              d \cdot (c_y - y) / f_y \\
              -d                      \\
              1
    \end{pmatrix}
\end{equation}
as evident from figure~\ref{fig:intrinsics}.

As the resulting point is in a local space relative to the camera $K_l$, it needs to be transformed into world space $K_w$.
To transform it into world space the model matrix is retrieved by calling
\texttt{cameraPoseAnchor.getPose().toMatrix(modelMatrix, 0)}.
%The model matrix is a transformation matrix that transforms points from a local space to world space.
By multiplying the model matrix $T_{wl}$ with $p_l$, the point is transformed into world space.
\begin{equation}
    p_w = T_{wl} \cdot p_l
\end{equation}

\subsection{Inserting new points into the point cloud}

With all pixels from the depth image transformed to world coordinates, the final step involves adding these points to the point cloud.
However, simply storing a list of all points and appending new data to it is not feasible.
As a new depth image is captured every frame, the number of points would grow endlessly.
This approach would also result in a massive number of points representing the same point on real-world geometry,
but with slightly different position values,
as subsequent depth images will show the same real-world geometry from different angles, with depth values varying slightly.
To address this issue, a spatial data structure that partitions the space into smaller regions and
allows to store only one point per region can be used.
One possible approach is to use a three-dimensional grid, where each cell represents a region in space.
Alternatively, a more advanced approach involves utilizing a spatial data structure such as an octree,
which can provide improved performance.

\paragraph{Octree}
An octree is a spatial tree data structure where each node has exactly eight children.
The tree can be used to sparsely partition three-dimensional space into smaller cubes and allows for efficient
insertion and traversal in logarithmic time.
"For the definition a simple recursive splitting of [cubes] is continued until there is only one point in a [cube]."
\parencite{gabriel_zachmann_geometric_2002}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{images/octree}
    \caption{Octree}
    \label{fig:octrree}
\end{figure}

\subsubsection{Fixed Depth Octree}
A straightforward approach to utilize an octree for point cloud storage is to use a variation of the octree with a fixed depth.
Points are always inserted at the defined depth of the tree, creating all nodes up to that depth if they do not yet exist.
The center coordinate of the leaf node can then be inferred as the point in space,
thus saving the coordinates of the point explicitly is not required.
Instead, to find the coordinates of a given node, the tree needs to be traversed while keeping track of
the extent and center coordinate of the current node.
This approach will naturally provide quantization of the point cloud, with the resolution of the point cloud
determined by the depth of the octree and extend of the root.
This is functionally equal to a three-dimensional grid,
but with the advantage of logarithmic time complexity for insertion and traversal.
Using this approach, duplicate points are removed and adjusting the resolution of the octree
allows for fine-tuning of the threshold distance between points under which points are considered duplicates.

One advantage of an octree with a fixed width is simplicity, as points are always inserted at the same depth and deletion of points is not required.
The biggest drawback is that the resolution of the point cloud is fixed and needs to be chosen beforehand.
As the accuracy of the depth images differ between devices and other conditions like lighting and texture of the captured surface,
it is difficult to choose a fixed resolution that works for all cases:

When using a resolution that is too high, some surfaces might be detected as multiple surfaces instead of one.
This is due to the same point being detected multiple times with slightly different depth values.
If the uncertainty of the depth data is too high for the chosen resolution,
points representing the same point in space will be added to neighboring cells of the octree.
In the case of planes, this will lead to points in multiple cells across the normal of the plane,
(increasing the thickness of the plane).
The RANSAC algorithm will then detect multiple planes instead of one.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{images/aliasing}
    \caption{Example of aliasing, distance between the line and cell centers varies across cells}
    \label{fig:aliasing}
\end{figure}

Quantization might also lead to worse detection results for surface that do not align with the axis of the octree.
For example, a plane that is tilted by a couple of degrees will lead to aliasing,
meaning that the distance between the points and the fitted plane will vary across the plane, as demonstrated in figure~\ref{fig:aliasing}.

\subsubsection{Octree with neighborhood condition}
%todo
To utilize an octree for point cloud storage, the point coordinates and its data is saved in the leaf nodes of the octree.
In this case, each node of the octree saves the coordinate of the point and its confidence value.
When inserting a new point with a certain confidence value, a range query with a radius calculated from the confidence value
is performed to find all nodes within a certain distance of the point to be inserted.
If no node is found, a new leaf node is created and the point is inserted.
If a node is found with a lower confidence value, the new point is inserted and the old node is removed.
If a node is found with a higher confidence value, the new point is not inserted and discarded.
Using this approach, duplicate points are removed and adjusting the multiplier for the radius of the range query
allows for fine-tuning of the threshold under which points are considered duplicates.

\paragraph{Range Query}
To check if an octree node, which is an axis-aligned bounding box (AABB) with equal sides, and a sphere intersect,
the square distance between the center of the sphere and the closest point on the AABB is calculated.
If the square distance is smaller than the square of the radius of the sphere, the sphere and the AABB intersect.

Calculating the square distance between a sphere and the closest point on the AABB can be achieved by summing up the
squared distance in each dimension:
If the sphere's center is outside the extent of the box on a given axis,
the distance is the amount by which it exceeds the box's boundary; otherwise, the distance is zero.
In $n$ dimensions, this can be expressed as
\begin{equation}
    d^2 = \sum_{i=1}^{n} \left\{
    \begin{array}{ll}
    (C_i - B_i - s)
        ^2                & \text{if } C_i > (B_i + s) \\
        (C_i - B_i + s)^2 & \text{if } C_i < (B_i - s) \\
        0                 & \text{otherwise}
    \end{array}
    \right\}
\end{equation}
where $C$ is the center of the sphere, $B$ is the center of the AABB, and $s$ is the half-size of the AABB\@.
\parencite{glassner_graphics_1994}

\paragraph{Deleting nodes}
Deleting nodes from an octree is a non-trivial task~\parencite{samet_design_1989, finkel_quad_1974}, as it may require restructuring the tree to maintain the octree properties.
\citeauthor{finkel_quad_1974}~\parencite{finkel_quad_1974} suggest reinserting all child nodes of the deleted node,
while [] propose a more efficient method, that tries to replace the deleted node with a suited node, such that the octree properties are maintained.
For simplicity, the first approach is used.
In addition, an optimization is made:
As nodes are only deleted when a new point is inserted with a higher confidence value,
it is possible to simply update the old node with the new position, in case the octree properties are not violated.
This is the case when the new point is within the same cell as the old point.
The tree is first traversed once to check if there exists such a node in the search radius.
If such a node is found, the old node is updated and the new point is not inserted.
If such a node is not found, the old node is deleted as by~\cite{finkel_quad_1974} and the new point is inserted.


\section{Detecting Primitives using RANSAC}

\parencite{schnabel_efficient_2007}


\section{Rendering the primitives}\label{sec:rendering-the-primitives}
The primitives are stored in world space, as in relative to the worlds' origin.
To render the primitives and overlay them onto the camera feed,
transforming the points from world space to screen space is required.

\subsection{OpenGL Rendering Pipeline}
This section provides an overview of the OpenGL rendering pipeline and the necessary steps to render the primitives.
All subsections are based on the book~\citetitle{de_vries_learn_2020} by \citeauthor{de_vries_learn_2020}~\parencite{de_vries_learn_2020}.

\subsubsection{Polygon Meshes}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.70\textwidth]{images/Mesh_overview}
    \caption{Mesh overview}
    \label{fig:mesh-overview}
\end{figure}

Rendering an object is achieved by creating a polygon mesh, which is a collection of vertices that define the faces of it.
The vertices are connected by edges to form polygons, which in turn create the surface of the object.
Polygons can be of any shape, but need to be planar, meaning that all vertices lie on the same plane.
The most commonly used type of polygon is the triangle, as it is the simplest polygon that can define a surface and is guaranteed to be planar.
The graphics pipeline is also optimized for triangles, as they are easy to rasterize and interpolate.
Figure~\ref{fig:mesh-overview} shows the relationship between vertices, edges, and polygons in a mesh. \cite{dorner_virtual_2019}

Polygon meshes can be represented many ways.
The simplest representation is a list of vertices, where the vertices are stored in a specific order to form the polygons.
In OpenGL, this can be achieved by using the \texttt{glDrawArrays} function,
which takes a buffer of vertices and draws them as triangles.
The problem with this approach is that in meshes vertices are often shared between multiple polygons,
as is apparent in figure~\ref{fig:mesh-overview}.
To render the cube in the figure, only 8 vertices are required,
but since each of the 6 faces is rendered with two triangle,
a total of 36 vertices would be required when using this approach.~\cite{dorner_virtual_2019, de_vries_learn_2020}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.60\textwidth]{images/index_buffer}
    \caption{The concept of indexing. VBO in blue, EBO in green.}
    \label{fig:index-buffer}
\end{figure}

A common solution to this problem is to use a data structure known as the indexed face-set, which consists of two lists -- one for the vertices and one for the indices.
The vertices list contains all unique vertices of the mesh, while the indices list contains the indices of the vertices that form the polygons.
In OpenGL, this can be achieved by using the \texttt{glDrawElements} function, which requires two buffers to be bound
-- one for the vertices, commonly referred as VBO (Vertex Buffer Object), and one for the indices, commonly referred as EBO (Element Buffer Object).
The concept of indexing is illustrated in figure~\ref{fig:index-buffer}.~\cite{dorner_virtual_2019, de_vries_learn_2020}

%the cameras origin (\textit{Clip Space}) with visible coordinates between -1 and 1 (normalized device coordinates) in all dimensions (frustum).
%Coordinates outside the frustum are clipped.
%Perspective projection can be applied here, more details in the next subsection.

%One exception is the viewport transformation,
%which is applied by OpenGL automatically based on the viewport settings provided via the \texttt{glViewport} function.

\subsubsection{Shaders}\label{subsec:shaders}

This section provides an overview of shaders and their role in the rendering pipeline as detailed by~\cite{de_vries_learn_2020}.

Shaders are isolated programs that run on the GPU and can be used to render objects.
In OpenGL, they are written in the OpenGL Shading Language (GLSL).
Two types of shaders are required to render an object: Vertex shaders and fragment shaders.
See figure~\ref{fig:graphics-pipeline} for an overview of the graphics pipeline.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.70\textwidth]{images/graphics-pipeline}
    \caption{Graphics Pipeline. Blue boxes represent programmable stages. The Geometry Shader is optional and not covered in this thesis.}
    \label{fig:graphics-pipeline}
\end{figure}

Vertex shaders are executed for each vertex defined in the vertex buffer, which is defined on the CPU and
passed to the GPU by copying the data to the GPU's memory, e.g.\ by using the \texttt{glBufferData} function.
This data can contain any attributes of the vertex, such as position, color, texture coordinates, or normals.
The vertex shader is then executed for each vertex and can be used to transform the vertex position between different coordinate systems,
or manipulate the vertex attributes on a per-vertex basis.
To pass data back to the pipeline, the vertex shader can define \texttt{out} variables.
In case of the vertex position, the output position is passed back to the
pipeline by assigning it to the \texttt{gl\_Position} variable using homogeneous coordinates.
OpenGL expects the vertex position to be in normalized device coordinates (NDC),
which range from -1 to 1 in all dimensions~\cite{dorner_virtual_2019}:
\begin{equation}
    (x, y, z) \in [-1, 1] \times [-1, 1] \times [-1, 1]
\end{equation}
Note that in other implementations, the cube might be defined with $z \in [0, 1]$.
All coordinates outside this cube are considered outside the view of the camera and subsequently clipped,
which means they will not be rendered.

The graphics pipline then uses the NDC and performs the perspective divide,
which divides the $x$, $y$, and $z$ coordinates by the $w$ coordinate of the homogenous coordinates,
resulting in a reduction of dimension from 4 to 3.
Then, the viewport transformation, which maps the NDC to screen space,
parameterized by the screen width and height in pixels, is performed.
Rasterization and interpolation of the vertex positions alongside all other vertex attributes
is then performed across the primitive.
The resulting elements are called fragments.
The fragment shader is then executed for each rasterized fragment of the primitive and is expected to
output a color by setting the \texttt{out vec4 FragColor} variable.
The screen position of the fragment and all variables that were passed as \texttt{out} from the
vertex shader can also be accessed in the fragment shader, with their values interpolated across the primitive.
This allows for smooth color transitions or texture mapping across the primitive.
Before the final color is written to the framebuffer,
a depth test is performed to determine if the fragment is visible or covered by fragments from other primitives,
in which case it is discarded.


\subsubsection{Perspective Projection in OpenGL}

As discussed in section~\ref{sec:perspective-projection}, perspective projection is used
to simulate the effect of objects appearing smaller the further away they are from the viewer.
This transformation from 3D space to a 2D projection is handled by a projection matrix.
In OpenGL rendering however, the projection matrix does not directly transform from view-space to screen-space.
Instead, an intermediate coordinate system known as clip space is introduced, as shown in figure~\ref{fig:coordinate-systems-with-clip-space}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{images/coordinate_systems}
    \caption{Extension of coordinate systems by clip space}
    \label{fig:coordinate-systems-with-clip-space}
\end{figure}
Coordinates in clip space are in normalized device coordinates (NDC), which is a cube
with coordinates ranging from -1 to 1 in all dimensions, as discussed in the previous section.

%A vertex shader can be used to transform the vertices from model space to screen space using the
%model, view, and projection matrices.
%To achieve this, the Model-View-Projection matrix (MVP) matrix can be passed as a \textit{uniform} to the vertex shader,
%which means it is the same for all vertices.
%The MVP matrix is the result of multiplying the model matrix, view matrix, and projection matrix.
%As the MVP matrix is the same for all vertices of an object, it can be calculated on the CPU and passed to the GPU as a uniform.
%The vertex position is then multiplied by the MVP matrix to transform it into clip space.

The projection matrix defines a frustum, which, depending on the projection type, can be a truncated pyramid or a cube,
that defines the volume of space visible through the camera lens.
In the case of the perspective projection, the frustum is a truncated pyramid, as shown in figure~\ref{fig:perspective}.
The parameters of the frustum are defined by the field of view, aspect ratio, and near and far clipping planes.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.50\textwidth]{images/perspective}
    \caption{Frustrum defined by perspective projection}
    \label{fig:perspective}
\end{figure}
Multiplication of the vertex position with the projection matrix transforms the vertex into clip space.
"The projection matrix [\ldots] also manipulates the w value of each vertex coordinate in such a way
that the further away a vertex coordinate is from the viewer, the higher this w component becomes"~\parencite{de_vries_learn_2020}.
When the coordinates are later divided by $w$ in the perspective divide,
it results in the desired perspective scaling effect.
Points closer to the viewer have a smaller $w$ and are less affected by the divide,
while points further away have a larger $w$ and are reduced in size more significantly.


\subsection{Rendering planes}
The RANSAC algorithm provides the parameterization of any detected plane using a normal vector $n$ and the point $p$ relative the worlds origin.
Using OpenGL, a plane can be rendered by creating two triangles composed of 3 vertices each, with two corner vertices shared between the triangles.
To render a plane from the parameterization, one can first find two arbitrary vectors $u$ and $v$
that are perpendicular to each-other and to the normal vector $n$.
Using $u$ and $v$, the four corner vertices of the plane can then be calculated by adding and subtracting $u * size / 2$ and $v * size / 2$ from the point $p$.

\paragraph{Calculating an arbitrary perpendicular vector}
The cross product of two vectors $a$ and $b$ is a vector that is perpendicular to both $a$ and $b$,
as long as $a$ and $b$ are not parallel.
To calculate an arbitrary perpendicular vector to a given vector $n$, one can use any of the 3 basis vectors ${b_1}$, ${b_2}$, and ${b_3}$.
Choosing any of the basis vectors that is not parallel to $n$ will result in a perpendicular vector.
To minimize floating point errors, which are largest for planes where $n$ almost aligns with the chosen basis vector,
the basis vector with the smallest dot product with $n$ can be chosen.
The normalized cross product of two vectors $n$ and $b_{smallest}$ then yields a perpendicular vector to $n$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\linewidth]{images/renderedPlane}
    \caption{Rendered RANSAC plane with size of 2*2m}
\end{figure}

\subsection{Constraining planes to the area where the points are located}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/bounding-volumes}
    \caption{Most commonly used bounding volumes}
\end{figure}

To constrain the planes to the area where the points are located, a bounding volume can be used.
A bounding volume is a geometric shape that encloses a set of points or other shapes,
and is often used to simplify collision detection or culling.
Common bounding volumes include axis-aligned bounding boxes (AABB), spheres, or oriented bounding boxes (OBB).
A more complex bounding volume is the convex hull, which is the smallest convex polygon that contains all the points.
\cite{gabriel_zachmann_geometric_2002}
A rubber band can be used to illustrate the concept: If a rubber band is stretched around a set of points,
the convex hull is the shape the rubber band takes, as illustrated in figure~\ref{fig:convex-hull}.
As the convex hull is the most accurate commonly used bounding volume,
it will be used for constraining the planes going forward.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.25\linewidth]{images/ConvexHull}
    \caption{Rubber band analogy for the Convex Hull}
\end{figure}

\subsubsection{Convex Hull Algorithms}
There are multiple algorithms to calculate the convex hull of a set of points.
In~\citeyear{graham_efficient_1972}~\cite{graham_efficient_1972} proposed an algorithm to calculate the convex hull of $n$ points in $O(n \log n)$ time.
\cite{andrew_another_1979} later proposed a variation of the algorithm in~\citeyear{andrew_another_1979}, which will be explained here.
More recent algorithms solve the problem in $O(n \log h)$ time, where $h$ is the number of points on the convex hull, but are more complex to implement.



Andrew's Monotone Chain Algorithm as proposed by~\cite{andrew_another_1979} ...

%todo: Explain the algorithm

\subsubsection{Rendering the plane constrained by the convex hull}
To render a plane constrained by the convex hull,
a triangle mesh can be created with triangles consisting of two subsequent triangles of the hull vertices and the centroid of the hull as the third vertex each.
Figure~\ref{fig:convex-hull} shows the result of the convex hull algorithm implemented into the application.
Prior to detecting the planes, the smartphone has been moved around the scene to capture the points from different angles.

\begin{figure}[ht!]

    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/points}
        \caption{Point data}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{python/plots/hull/hull}
        \caption{Convex Hull}
    \end{subfigure}%

    \vspace{0.5em}

    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/hull}
        \caption{Rendered Primitive}
    \end{subfigure}%
    \caption{Convex Hull with real data}
    \label{fig:convex-hull}
\end{figure}





\parencite{graham_efficient_1972, andrew_another_1979}

