\chapter{Solution Details}

Depth information is collected using the ARCore Raw Depth API via depth images and confidence images.
Low confidence points under a certain threshold are filtered out.
Each pixel of the depth image is then projected into three-dimensional world-space, utilizing the camera's intrinsics.
An octree is then use to remove duplicate points as new data arrives, and to quantize the points cloud.
The RANSAC algorithm is then applied to the point cloud to detect geometric primitives.


\section{Capturing Depth Images}

\subsection{Depth from Motion}
Depth from motion is a technique that estimates depth information from a sequence of images~\parencite{valentin_depth_2018}.

"Depth data becomes available when the user moves their device. The algorithm can get robust, accurate depth estimates from 0 to 65 meters away. The most accurate results come when the device is half a meter to about five meters away from the real-world scene. Experiences that encourage the user to move their device more will get better and better results."
\parencite{google_llc_arcore_doc}

\subsection{ARCore Depth APIs}
Google ARCore provides two APIs to access depth information: the Depth API and the Raw Depth API\@.
Both APIs provide depth information for a given frame of a camera image using depth images, but they differ in the level of detail they provide~\parencite{google_llc_arcore_doc}:

The Raw Depth API provides depth images and confidence images, where some pixels may not have any depth information.
The depth image provides the distance from the camera of a given pixel in millimeters.
The confidence image indicates the reliability of the depth information for each pixel, ranging from 0 (no confidence) to 255 (high confidence).

In contrast, the Depth API provides a single depth image, where each pixel has a depth value.
To achieve this, values for pixels without depth information are interpolated.
No confidence image is provided.

\begin{figure}[ht!]
    \centering
    % First row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_full-depth-image}
        \caption{Full Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_camera-image}
        \caption{Camera image}
    \end{subfigure}%

    % Optional: Adjust or remove vertical spacing between the rows
    \vspace{0.5em}

    % Second row
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-image}
        \caption{Raw Depth API depth image}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/depth_raw-depth-confidence-image}
        \caption{Raw Depth API confidence image}
    \end{subfigure}%

    \caption{Full Depth vs. Raw Depth}
    \label{fig:depth-api-images}
\end{figure}


The Depth API is preferred in cases where it is crucial to have a depth value for every pixel, such as calculating if an object should be occluded by the scene in an AR application,
while the Raw depth API is preferred if accuracy of the depth information is crucial. \parencite{google_llc_arcore_doc}
As accuracy is crucial for primitive detection and depth information for every pixel is not required, the Raw Depth API is used in this project.

\section{Building the point cloud (PC)}
As RANSAC is a point-based algorithm, we first need to convert the depth image to a point cloud.
The process consists of three steps.
\begin{enumerate}
    \item Filtering low confidence points
    \item Transforming depth image pixels to world coordinate points
    \item Inserting new points into the point cloud
\end{enumerate}

\subsection{Filtering low confidence points}
The confidence image provided by the Raw Depth API can be used to filter out points with low confidence.
A threshold value is set, below which points are discarded.

\subsection{Transforming depth image pixels to world coordinate points}
To convert the point into world coordinates, first the camera intrinsics are used to
project the point into camera space.
Then, the model matrix is used to transform the point into world space.
\parencite{google_llc_codelab_raw_depth,google_llc_arcore_doc}

These transformations can be seen as analog to the transformations needed to
render an object in world space onto the screen, as described in section~\ref{sec:rendering-the-primitives}.
In this case, the transformation is applied backwards, i.e.\ in the opposite direction of the arrows
in the figure~\ref{fig:coordinate-systems}, starting from clip space (4) to view space (3) to world space (2).


%"Given point $A$ on the observed real-world geometry and a 2D point a representing the same point in the depth image,
%the value given by the Depth API at a is equal to the length of $CA$ projected onto the principal axis.
%This can also be referred as the z-coordinate of $A$ relative to the camera origin $C$." \parencite{google_llc_arcore_doc}
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.75\textwidth]{images/depth-values-diagram}
%    \caption{}
%    \label{fig:}
%\end{figure}
%%codeblock with the formula
%\begin{lstlisting}[language=Java]
%float fx =
%  cameraTextureIntrinsics.getFocalLength()[0] * depthWidth / intrinsicsDimensions[0];
%float fy =
%  cameraTextureIntrinsics.getFocalLength()[1] * depthHeight / intrinsicsDimensions[1];
%float cx =
%  cameraTextureIntrinsics.getPrincipalPoint()[0] * depthWidth / intrinsicsDimensions[0];
%float cy =
%  cameraTextureIntrinsics.getPrincipalPoint()[1] * depthHeight / intrinsicsDimensions[1];
%pointCamera[0] = depthMeters * (x - cx) / fx;
%pointCamera[1] = depthMeters * (cy - y) / fy;
%pointCamera[2] = -depthMeters;
%pointCamera[3] = 1;
%\end{lstlisting}

%This results in a point in camera space,
%which can then be transformed into world space by multiplying it with the model matrix.



\subsection{Inserting new points into the point cloud}

The last step is to add new points to the point cloud.
As new depth images are captured every frame, we can not simply store a list of all points to append the new data to.
This would lead to a massive amount of points that represent the same point in space, with slightly different depth values.
To circumvent this, we can use a data structure that allows us to partition the space into smaller regions, and only store one point per region.
A simple approach would utilize a grid, where each cell represents a region in space, while a more sophisticated approach
would use a spatial data structure like an octree to improve performance.

\subsubsection{Octree}
An octree is a tree data structure where each node has exactly eight children.
The tree can be used to sparsely partition three-dimensional space into smaller cubes and allows for efficient
insertion and traversal in logarithmic time.
"For the definition a simple recursive splitting of [cubes] is continued until there is only one point in a [cube]."
\parencite{gabriel_zachmann_geometric_2002}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{images/octree}
    \caption{Octree}
    \label{fig:octrree}
\end{figure}

To utilize an octree for point cloud storage, the point coordinates and its data is saved in the leaf nodes of the octree.
In this case, each node of the octree saves the coordinate of the point and its confidence value.
When inserting a new point with a certain confidence value, a range query with a radius calculated from the confidence value
is performed to find all nodes within a certain distance of the point to be inserted.
If no node is found, a new leaf node is created and the point is inserted.
If a node is found with a lower confidence value, the new point is inserted and the old node is removed.
If a node is found with a higher confidence value, the new point is not inserted and discarded.
Using this approach, duplicate points are removed and adjusting the multiplier for the radius of the range query
allows for fine-tuning of the threshold under which points are considered duplicates.

A different approach is to use a variation of the octree with a fixed depth.
Points are always inserted at the defined depth of the tree, creating all nodes up to that depth if they do not yet exist.
The center coordinate of the leaf node can then be inferred as the point in space,
thus saving the coordinates of the point explicitly is not required.
Instead, to find the coordinates of a given node, the tree needs to be traversed while keeping track of
the extent and center coordinate of the current node.
This approach will naturally provide quantization of the point cloud, with the resolution of the point cloud
determined by the depth of the octree and extend of the root node.
Using this approach, duplicate points are removed and adjusting the resolution of the octree
allows for fine-tuning of the threshold distance between points under which points are considered duplicates.

One disadvantage of the latter approach is that the confidence value of the points are not considered,
thus a higher threshold for filtering out low confidence points is required.
This can be mitigated by extending the second approach by a technique similar to the first approach,
where the confidence value is stored with the point and a range query is performed to replace lower confidence values.

The performance of the RANSAC algorithm with both approaches will be evaluated in chapter~\ref{subsec:eval-point-clouds}.


\section{Detecting Primitives using RANSAC}

\parencite{schnabel_efficient_2007}


\section{Rendering the primitives}\label{sec:rendering-the-primitives}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{images/coordinate_systems}
    \caption{Coordinate systems}
    \label{fig:coordinate-systems}
\end{figure}

\parencite{de_vries_learn_2020}

\subsection{Preparing the data on the CPU}

\subsubsection{Restraining the primitives}

\paragraph{Cross-section => Floor and Walls}

\paragraph{Using a bounding box / extend of points}

\subsubsection{Building a mesh}

\subsection{Rendering on the GPU (shaders)}
