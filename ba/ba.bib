
@article{kaiser_survey_2019,
	title = {A Survey of Simple Geometric Primitives Detection Methods for Captured 3D Data},
	volume = {38},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13451},
	doi = {10.1111/cgf.13451},
	abstract = {The amount of captured 3D data is continuously increasing, with the democratization of consumer depth cameras, the development of modern multi-view stereo capture setups and the rise of single-view 3D capture based on machine learning. The analysis and representation of this ever growing volume of 3D data, often corrupted with acquisition noise and reconstruction artefacts, is a serious challenge at the frontier between computer graphics and computer vision. To that end, segmentation and optimization are crucial analysis components of the shape abstraction process, which can themselves be greatly simpliﬁed when performed on lightened geometric formats. In this survey, we review the algorithms which extract simple geometric primitives from raw dense 3D data. After giving an introduction to these techniques, from the acquisition modality to the underlying theoretical concepts, we propose an application-oriented characterization, designed to help select an appropriate method based on one’s application needs and compare recent approaches. We conclude by giving hints for how to evaluate these methods and a set of research challenges to be explored.},
	pages = {167--196},
	number = {1},
	journaltitle = {Computer Graphics Forum},
	shortjournal = {Computer Graphics Forum},
	author = {Kaiser, Adrien and Ybanez Zepeda, Jose Alonso and Boubekeur, Tamy},
	urldate = {2024-01-16},
	date = {2019-02},
	langid = {english},
	keywords = {3D data, computational geometry, data fitting, geometric primitives, I.3.5 Computing Methodologies/Computer Graphics: Computational Geometry and Object Modelling—Curve, shape abstraction, shape analysis, solid and object representations, surface, notion, fundamentals},
	file = {Kaiser et al. - 2019 - A Survey of Simple Geometric Primitives Detection .pdf:/home/hugop/Zotero/storage/YME9444A/Kaiser et al. - 2019 - A Survey of Simple Geometric Primitives Detection .pdf:application/pdf;Snapshot:/home/hugop/Zotero/storage/97E2ZWPG/cgf.html:text/html},
}

@article{schnabel_efficient_2007,
	title = {Efficient {RANSAC} for Point‐Cloud Shape Detection},
	volume = {26},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2007.01016.x},
	doi = {10.1111/j.1467-8659.2007.01016.x},
	abstract = {In this work we present an automatic algorithm to detect basic shapes in unorganized point clouds. The algorithm decomposes the point cloud into a concise, hybrid structure of inherent shapes and a set of remaining points. Each detected shape serves as a proxy for a set of corresponding points. Our method is based on random sampling and detects planes, spheres, cylinders, cones and tori. For models with surfaces composed of these basic shapes only, e.g. {CAD} models, we automatically obtain a representation solely consisting of shape proxies. We demonstrate that the algorithm is robust even in the presence of many outliers and a high degree of noise. The proposed method scales well with respect to the size of the input point cloud and the number and size of the shapes within the data. Even point sets with several millions of samples are robustly decomposed within less than a minute. Moreover the algorithm is conceptually simple and easy to implement. Application areas include measurement of physical parameters, scan registration, surface compression, hybrid rendering, shape classiﬁcation, meshing, simpliﬁcation, approximation and reverse engineering.},
	pages = {214--226},
	number = {2},
	journaltitle = {Computer Graphics Forum},
	shortjournal = {Computer Graphics Forum},
	author = {Schnabel, R. and Wahl, R. and Klein, R.},
	urldate = {2024-02-06},
	date = {2007-06},
	langid = {english},
	keywords = {@jenke, notion, fundamentals},
	file = {Schnabel et al. - 2007 - Efficient RANSAC for Point‐Cloud Shape Detection.pdf:/home/hugop/Zotero/storage/ZCAI22CK/Schnabel et al. - 2007 - Efficient RANSAC for Point‐Cloud Shape Detection.pdf:application/pdf},
}

@article{chatzopoulos_mobile_2017,
	title = {Mobile Augmented Reality Survey: From Where We Are to Where We Go},
	volume = {5},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/7912316},
	doi = {10.1109/ACCESS.2017.2698164},
	shorttitle = {Mobile Augmented Reality Survey},
	abstract = {The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality ({MAR}) from science fiction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated {MAR} applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of {MAR}, we present a categorization of the application fields together with some representative examples. Next, we introduce the reader to the user interface and experience in {MAR} applications and continue with the core system components of the {MAR} systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any {MAR} application and the network connectivity of the devices that run {MAR} applications together with its importance to the performance of the application. We continue with the importance of data management in {MAR} systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.},
	pages = {6917--6950},
	journaltitle = {{IEEE} Access},
	author = {Chatzopoulos, Dimitris and Bermejo, Carlos and Huang, Zhanpeng and Hui, Pan},
	urldate = {2024-02-09},
	date = {2017},
	note = {Conference Name: {IEEE} Access},
	keywords = {Augmented reality, Cloud computing, human computer interaction, Mobile augmented reality, Mobile communication, mobile computing, Mobile computing, Sensors, Smart phones, notion, exploration},
	file = {IEEE Xplore Abstract Record:/home/hugop/Zotero/storage/PWHJTFSN/7912316.html:text/html;IEEE Xplore Full Text PDF:/home/hugop/Zotero/storage/VFXVAYL9/Chatzopoulos et al. - 2017 - Mobile Augmented Reality Survey From Where We Are.pdf:application/pdf},
}

@online{google_llc_codelab_raw_depth,
	title = {{ARCore} Raw Depth},
	url = {https://codelabs.developers.google.com/codelabs/arcore-rawdepthapi},
	abstract = {This codelab shows you steps for building an {ARCore} application using the new Raw Depth {API}.},
	titleaddon = {Google Codelabs},
	author = {{Google LLC} and Turner, Eric},
	urldate = {2024-02-09},
	date = {2022-05-11},
	langid = {english},
	keywords = {notion, fundamentals},
	file = {ARCore Raw Depth Part 1.html:/home/hugop/Zotero/storage/5UGM64DA/ARCore Raw Depth Part 1.html:text/html;ARCore Raw Depth Part 2.html:/home/hugop/Zotero/storage/FNB27VS8/ARCore Raw Depth Part 2.html:text/html;ARCore Raw Depth Part 3.html:/home/hugop/Zotero/storage/M4ALWJAJ/ARCore Raw Depth Part 3.html:text/html},
}

@online{google_llc_arcore_doc,
	title = {{ARCore} Developer Documentation},
	url = {https://developers.google.com/ar/develop},
	abstract = {Explore an overview of Google\&\#39;s platform for building augmented reality experiences.},
	titleaddon = {Google for Developers},
	author = {{Google LLC}},
	urldate = {2024-02-09},
	langid = {english},
	keywords = {notion, fundamentals},
	file = {Depth adds realism  ARCore.html:/home/hugop/Zotero/storage/54R9ZHDN/Depth adds realism  ARCore.html:text/html;Overview of ARCore and supported development envir.html:/home/hugop/Zotero/storage/QIJMQ9HN/Overview of ARCore and supported development envir.html:text/html;Use Raw Depth in your Android app  ARCore.html:/home/hugop/Zotero/storage/WFJ2QVFY/Use Raw Depth in your Android app  ARCore.html:text/html},
}

@book{de_vries_learn_2020,
	location = {Erscheinungsort nicht ermittelbar},
	title = {Learn {OpenGL} - Graphics programming: Learn modern {OpenGL} graphics programming in a step-by-step fashion},
	isbn = {978-90-90-33256-7},
	shorttitle = {Learn {OpenGL}},
	abstract = {Graphics programmers are often coined the 'wizards' of the game industry. As every magician knows, terms like wizardry and magic are often somewhat exaggerated. Yet, there is a certain charm to graphics programming: the ability to conjure up complete living worlds at our fingertips. Learn {OpenGL} will teach you the basics, the intermediate, and tons of advanced knowledge, using modern (core-profile) {OpenGL}. The aim of this book is to show you all there is to modern {OpenGL} in an easy-to-understand fashion, with clear examples and step-by-step instructions, while also providing a useful reference for later studies. After years of continuous work and improvements on the accompanying website learnopengl.com, with the help of thousands of readers, its content has been professionally revised for this physical copy you now find in your hands. Graphics programming isn't as hard as many people make it out to be... you just need to start},
	pagetotal = {522},
	publisher = {Kendall \& Welling},
	author = {de Vries, Joey},
	date = {2020},
	keywords = {notion, fundamentals},
	file = {Vries - 2020 - Learn OpenGL - Graphics programming Learn modern .pdf:/home/hugop/Zotero/storage/4ZL4T74G/Vries - 2020 - Learn OpenGL - Graphics programming Learn modern .pdf:application/pdf},
}

@article{tarsha-kurdi_hough-transform_2007,
	title = {Hough-Transform and Extended {RANSAC} Algorithms for Automatic Detection of 3D Building Roof Planes from Lidar Data},
	abstract = {Airborne laser scanner technique is broadly the most appropriate way to acquire rapidly and with high density 3D data over a city. Once the 3D Lidar data are available, the next task is the automatic data processing, with major aim to construct 3D building models. Among the numerous automatic reconstruction methods, the techniques allowing the detection of 3D building roof planes are of crucial importance. Three main methods arise from the literature: region growing, Hough-transform and Random Sample Consensus ({RANSAC}) paradigm. Since region growing algorithms are sometimes not very transparent and not homogenously applied, this paper focuses only on the Hough-transform and the {RANSAC} algorithm. Their principles, their pseudocode - rarely detailed in the related literature - as well as their complete analyses are presented in this paper. An analytic comparison of both algorithms, in terms of processing time and sensitivity to cloud characteristics, shows that despite the limitation encountered in both methods, {RANSAC} algorithm is still more efficient than the first one. Under other advantages, its processing time is negligible even when the input data size is very large. On the other hand, Hough-transform is very sensitive to the segmentation parameters values. Therefore, {RANSAC} algorithm has been chosen and extended to exceed its limitations. Its major limitation is that it searches to detect the best mathematical plane among 3D building point cloud even if this plane does not always represent a roof plane. So the proposed extension allows harmonizing the mathematical aspect of the algorithm with the geometry of a roof. At last, it is shown that the extended approach provides very satisfying results, even in the case of very weak point density and for different levels of building complexity. Therefore, once the roof planes are successfully detected, the automatic building modelling can be carried out.},
	author = {Tarsha-Kurdi, Fayez and Landes, Tania and Grussenmeyer, Pierre},
	date = {2007},
	langid = {english},
	keywords = {notion, application},
	file = {Tarsha-Kurdi et al. - 2007 - Hough-Transform and Extended RANSAC Algorithms for.pdf:/home/hugop/Zotero/storage/8KWUKVUB/Tarsha-Kurdi et al. - 2007 - Hough-Transform and Extended RANSAC Algorithms for.pdf:application/pdf},
}

@article{haenel_integration_2022,
	title = {{INTEGRATION} {OF} {DEPTH} {MAPS} {FROM} {ARCORE} {TO} {PROCESS} {POINT} {CLOUDS} {IN} {REAL} {TIME} {ON} A {SMARTPHONE}},
	volume = {{XLIII}-B2-2022},
	doi = {10.5194/isprs-archives-XLIII-B2-2022-201-2022},
	abstract = {Real-world three-dimensional reconstruction is a project of long-standing interest in global computer vision. Many tools have emerged these past years to accurately perceive the surrounding world either through active sensors or through passive algorithmic methods. With the advent and popularization of augmented reality on smartphones, new visualization issues have emerged concerning the virtual experience. Especially a 3D model seems to be essential to provide more realistic {AR} effects including consistency of occlusion, shadow mapping or even collision between virtual objects and real environment. However, due to the huge computation of most of current approaches, most of these algorithms are working on a computer desktop or high-end smartphones. Indeed, the reconstruction scale is rapidly limited by the complexity of both computation and memory. Therefore, our study aims to find a relevant method to process real time reconstruction of close-range outdoor scenes such as cultural heritage or underground infrastructures in real time locally on a smartphone.},
	pages = {201--208},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Haenel, Raphaël and Semler, Quentin and Semin, E. and Grussenmeyer, Pierre and Alby, Emmanuel},
	date = {2022-05-30},
	keywords = {notion, application},
	file = {Full Text:/home/hugop/Zotero/storage/X97MCSAW/Haenel et al. - 2022 - INTEGRATION OF DEPTH MAPS FROM ARCORE TO PROCESS P.pdf:application/pdf},
}

@article{valentin_depth_2018,
	title = {Depth from motion for smartphone {AR}},
	volume = {37},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3272127.3275041},
	doi = {10.1145/3272127.3275041},
	abstract = {Augmented reality ({AR}) for smartphones has matured from a technology for earlier adopters, available only on select high-end phones, to one that is truly available to the general public. One of the key breakthroughs has been in low-compute methods for six degree of freedom (6DoF) tracking on phones using only the existing hardware (camera and inertial sensors). 6DoF tracking is the cornerstone of smartphone {AR} allowing virtual content to be precisely locked on top of the real world. However, to really give users the impression of believable {AR}, one requires mobile depth. Without depth, even simple effects such as a virtual object being correctly occluded by the real-world is impossible. However, requiring a mobile depth sensor would severely restrict the access to such features. In this article, we provide a novel pipeline for mobile depth that supports a wide array of mobile phones, and uses only the existing monocular color sensor. Through several technical contributions, we provide the ability to compute low latency dense depth maps using only a single {CPU} core of a wide range of (medium-high) mobile phones. We demonstrate the capabilities of our approach on high-level {AR} applications including real-time navigation and shopping.},
	pages = {193:1--193:19},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Valentin, Julien and Kowdle, Adarsh and Barron, Jonathan T. and Wadhwa, Neal and Dzitsiuk, Max and Schoenberg, Michael and Verma, Vivek and Csaszar, Ambrus and Turner, Eric and Dryanovski, Ivan and Afonso, Joao and Pascoal, Jose and Tsotsos, Konstantine and Leung, Mira and Schmidt, Mirko and Guleryuz, Onur and Khamis, Sameh and Tankovitch, Vladimir and Fanello, Sean and Izadi, Shahram and Rhemann, Christoph},
	urldate = {2024-02-16},
	date = {2018-12-04},
	keywords = {depth from motion, motion stereo, structure from motion, notion, fundamentals},
	file = {Full Text PDF:/home/hugop/Zotero/storage/R4BK7KU5/Valentin et al. - 2018 - Depth from motion for smartphone AR.pdf:application/pdf},
}

@article{azuma_survey_1997,
	title = {A Survey of Augmented Reality},
	abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
	author = {Azuma, Ronald T},
	date = {1997-08},
	langid = {english},
	keywords = {notion},
	file = {Azuma - A Survey of Augmented Reality.pdf:/home/hugop/Zotero/storage/JWAGTN8J/Azuma - A Survey of Augmented Reality.pdf:application/pdf},
}

@report{bhavsar_point_2022,
	title = {Point Cloud Room Estimation for 3D Mesh Placement},
	abstract = {Point clouds can help us easily determine different components within a room. In this paper, we demonstrate a {RANSAC} heuristic model to detect floors and ceilings by using an {RGB} and depth image, applying plane segmentation to the generated point cloud. We apply these predictions to place a 3D mesh chair within the room, using our results to scale and align our chair object. We do so by identifying free space within the floor’s point cloud, then translating the chair to that location. Results show that our model is capable of identifying empty floor space sufficient to place a chair mesh, in a variety of rooms, given adequate conditions. On the other hand, cluttered rooms and large horizontal objects, such as tables, pose a challenge for the heuristics used, and options to improve accuracy are discussed, as well as performance improvements to search point clouds efficiently.},
	author = {Bhavsar, Anand},
	date = {2022},
	langid = {english},
	keywords = {notion, application},
	file = {Bhavsar - Point Cloud Room Estimation for 3D Mesh Placement.pdf:/home/hugop/Zotero/storage/YEC5PAGW/Bhavsar - Point Cloud Room Estimation for 3D Mesh Placement.pdf:application/pdf},
}

@misc{zhao_object_2019,
	title = {Object Detection with Deep Learning: A Review},
	url = {http://arxiv.org/abs/1807.05511},
	doi = {10.48550/arXiv.1807.05511},
	shorttitle = {Object Detection with Deep Learning},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network ({CNN}). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
	number = {{arXiv}:1807.05511},
	publisher = {{arXiv}},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-tao and Wu, Xindong},
	urldate = {2024-02-25},
	date = {2019-04-16},
	eprinttype = {arxiv},
	eprint = {1807.05511 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/home/hugop/Zotero/storage/FDEVD8X4/Zhao et al. - 2019 - Object Detection with Deep Learning A Review.pdf:application/pdf;arXiv.org Snapshot:/home/hugop/Zotero/storage/NIFJ3BWH/1807.html:text/html},
}

@book{schmalstieg_augmented_2008,
	title = {Augmented Reality 2.0},
	isbn = {978-3-211-99177-0},
	abstract = {Augmented Reality ({AR}) was first demonstrated in the 1960s, but only recently have technologies emerged that can be used to
easily deploy {AR} applications to many users. Camera-equipped cell phones with significant processing power and graphics abilities
provide an inexpensive and versatile platform for {AR} applications, while the social networking technology of Web 2.0 provides
a large-scale infrastructure for collaboratively producing and distributing geo-referenced {AR} content. This combination of
widely used mobile hardware and Web 2.0 software allows the development of a new type of {AR} platform that can be used on a
global scale. In this paper we describe the Augmented Reality 2.0 concept and present existing work on mobile {AR} and web technologies
that could be used to create {AR} 2.0 applications.},
	pagetotal = {13},
	author = {Schmalstieg, Dieter and Langlotz, Tobias and Billinghurst, Mark},
	date = {2008-01-01},
	doi = {10.1007/978-3-211-99178-7_2},
	note = {Journal Abbreviation: Virtual Realities: Dagstuhl Seminar 2008
Pages: 37
Publication Title: Virtual Realities: Dagstuhl Seminar 2008},
	keywords = {/unread, notion},
	file = {Full Text PDF:/home/hugop/Zotero/storage/SYU9ZZTT/Schmalstieg et al. - 2008 - Augmented Reality 2.0.pdf:application/pdf},
}

@article{chen_overview_2019,
	title = {An overview of augmented reality technology},
	volume = {1237},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1237/2/022082},
	doi = {10.1088/1742-6596/1237/2/022082},
	abstract = {Augmented reality is a technology that combines virtual reality with reality. In recent years, the rapid development of augmented reality technology has aroused people's high attention. This paper first expounds the research and progress of augmented reality at home and abroad. Secondly, it introduces the key technologies, development tools and application of augmented reality in some fields. Finally, it looks forward to the future development trend of augmented reality technologies such as {AR} cloud.},
	pages = {022082},
	number = {2},
	journaltitle = {Journal of Physics: Conference Series},
	shortjournal = {J. Phys.: Conf. Ser.},
	author = {Chen, Yunqiang and Wang, Qing and Chen, Hong and Song, Xiaoyu and Tang, Hui and Tian, Mengxiao},
	urldate = {2024-02-25},
	date = {2019-06-01},
	langid = {english},
	keywords = {notion, exploration},
	file = {Chen et al. - 2019 - An overview of augmented reality technology.pdf:/home/hugop/Zotero/storage/4LGTEY3H/Chen et al. - 2019 - An overview of augmented reality technology.pdf:application/pdf},
}

@article{rabbi_survey_2013,
	title = {A Survey on Augmented Reality Challenges and Tracking Authors},
	author = {Rabbi, Ihsan and Ullah, Sehat},
	date = {2013},
	langid = {english},
	keywords = {notion, exploration},
	file = {Rabbi and Ullah - 2013 - A Survey on Augmented Reality Challenges and Track.pdf:/home/hugop/Zotero/storage/GB9FQ2YV/Rabbi and Ullah - 2013 - A Survey on Augmented Reality Challenges and Track.pdf:application/pdf},
}

@article{li_hybrid_2024,
	title = {Hybrid 3D Reconstruction of Indoor Scenes Integrating Object Recognition},
	volume = {16},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/16/4/638},
	doi = {10.3390/rs16040638},
	abstract = {Indoor 3D reconstruction is particularly challenging due to complex scene structures involving object occlusion and overlap. This paper presents a hybrid indoor reconstruction method that segments the room point cloud into internal and external components, and then reconstructs the room shape and the indoor objects in different ways. We segment the room point cloud into internal and external points based on the assumption that the room shapes are composed of some large external planar structures. For the external, we seek for an appropriate combination of intersecting faces to obtain a lightweight polygonal surface model. For the internal, we define a set of features extracted from the internal points and train a classification model based on random forests to recognize and separate indoor objects. Then, the corresponding computer aided design ({CAD}) models are placed in the target positions of the indoor objects, converting the reconstruction into a model fitting problem. Finally, the indoor objects and room shapes are combined to generate a complete 3D indoor model. The effectiveness of this method is evaluated on point clouds from different indoor scenes with an average fitting error of about 0.11 m, and the performance is validated by extensive comparisons with state-of-the-art methods.},
	pages = {638},
	number = {4},
	journaltitle = {Remote Sensing},
	author = {Li, Mingfan and Li, Minglei and Xu, Li and Wei, Mingqiang},
	urldate = {2024-02-25},
	date = {2024-01},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {notion, indoor modeling, lightweight model, model fitting, point cloud classification, exploration},
	file = {Full Text PDF:/home/hugop/Zotero/storage/LFKI8FMW/Li et al. - 2024 - Hybrid 3D Reconstruction of Indoor Scenes Integrat.pdf:application/pdf},
}

@article{fayolle_survey_2024,
	title = {A Survey of Methods for Converting Unstructured Data to {CSG} Models},
	volume = {168},
	issn = {0010-4485},
	url = {https://www.sciencedirect.com/science/article/pii/S0010448523001872},
	doi = {10.1016/j.cad.2023.103655},
	abstract = {The goal of this document is to survey existing methods for recovering or extracting {CSG} (Constructive Solid Geometry) representations from unstructured data such as 3D point-clouds or polygon meshes. We review and discuss related topics such as the segmentation and fitting of the input data. We cover techniques from solid modeling for the conversion of a polyhedron to a {CSG} expression and for the conversion of a B-rep to a {CSG} expression. We look at approaches coming from program synthesis, evolutionary techniques (such as genetic programming or genetic algorithm), and deep learning. Finally, we conclude our survey with a discussion of techniques for the generation of computer programs involving higher-level constructs, representations, and operations for representing solids.},
	pages = {103655},
	journaltitle = {Computer-Aided Design},
	shortjournal = {Computer-Aided Design},
	author = {Fayolle, Pierre-Alain and Friedrich, Markus},
	urldate = {2024-02-25},
	date = {2024-03-01},
	keywords = {notion, Constructive solid geometry, Fitting, Point-cloud and polygon soup to {CSG} conversion, Segmentation, Shape program synthesis, exploration},
	file = {ScienceDirect Snapshot:/home/hugop/Zotero/storage/22UT7QUA/S0010448523001872.html:text/html;Submitted Version:/home/hugop/Zotero/storage/KG2CUCGP/Fayolle and Friedrich - 2024 - A Survey of Methods for Converting Unstructured Da.pdf:application/pdf},
}

@article{kellner_reconstructing_2023,
	title = {Reconstructing Geometrical Models of Indoor Environments Based on Point Clouds},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/18/4421},
	doi = {10.3390/rs15184421},
	abstract = {In this paper, we present a workflow that combines supervised and unsupervised methods for the reconstruction of geometric models with architectural information from unordered 3D data. Our method uses a downsampling strategy to enrich features to provide scalability for large datasets, increase robustness, and be independent of the sensor used. A Neural Network is then used to segment the resulting point cloud into basic structures. This removes furniture and clutter and preserves the relevant walls, ceilings, floors, and openings. A 2D projection combined with a graph structure is used to find a Region of Interest within the cleaned point cloud, indicating a potential room. Each detected region is projected back into a 3D data patch to refine the room candidates and allow for more complex room structures. The resulting patches are fitted with a polygon using geometric approaches. In addition, architectural features, such as windows and doors, are added to the polygon. To demonstrate that the presented approach works and that the network provides usable results, even with changing data sources, we tested the approach in different real-world scenarios with different sensor systems.},
	pages = {4421},
	number = {18},
	journaltitle = {Remote Sensing},
	author = {Kellner, Maximilian and Stahl, Bastian and Reiterer, Alexander},
	urldate = {2024-02-25},
	date = {2023-01},
	langid = {english},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, notion, exploration, deep learning, point cloud, Scan-to-{BIM}, scene understanding},
	file = {Full Text PDF:/home/hugop/Zotero/storage/IED2EJTV/Kellner et al. - 2023 - Reconstructing Geometrical Models of Indoor Enviro.pdf:application/pdf},
}

@article{liu_sharp_2023,
	title = {Sharp Feature-Preserving 3D Mesh Reconstruction from Point Clouds Based on Primitive Detection},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/12/3155},
	doi = {10.3390/rs15123155},
	abstract = {High-fidelity mesh reconstruction from point clouds has long been a fundamental research topic in computer vision and computer graphics. Traditional methods require dense triangle meshes to achieve high fidelity, but excessively dense triangles may lead to unnecessary storage and computational burdens, while also struggling to capture clear, sharp, and continuous edges. This paper argues that the key to high-fidelity reconstruction lies in preserving sharp features. Therefore, we introduce a novel sharp-feature-preserving reconstruction framework based on primitive detection. It includes an improved deep-learning-based primitive detection module and two novel mesh splitting and selection modules that we propose. Our framework can accurately and reasonably segment primitive patches, fit meshes in each patch, and split overlapping meshes at the triangle level to ensure true sharpness while obtaining lightweight mesh models. Quantitative and visual experimental results demonstrate that our framework outperforms both the state-of-the-art learning-based primitive detection methods and traditional reconstruction methods. Moreover, our designed modules are plug-and-play, which not only apply to learning-based primitive detectors but also can be combined with other point cloud processing tasks such as edge extraction or random sample consensus ({RANSAC}) to achieve high-fidelity results.},
	pages = {3155},
	number = {12},
	journaltitle = {Remote Sensing},
	author = {Liu, Qi and Xu, Shibiao and Xiao, Jun and Wang, Ying},
	urldate = {2024-02-25},
	date = {2023-01},
	langid = {english},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, notion, mesh reconstruction, point clouds, primitive detection, sharp feature},
	file = {Full Text PDF:/home/hugop/Zotero/storage/7ZQFHRGQ/Liu et al. - 2023 - Sharp Feature-Preserving 3D Mesh Reconstruction fr.pdf:application/pdf},
}

@article{schnabel_octree-based_nodate,
	title = {Octree-based Point-Cloud Compression},
	abstract = {In this paper we present a progressive compression method for point sampled models that is speciﬁcally apt at dealing with densely sampled surface geometry. The compression is lossless and therefore is also suitable for storing the unﬁltered, raw scan data. Our method is based on an octree decomposition of space. The point-cloud is encoded in terms of occupied octree-cells. To compress the octree we employ novel prediction techniques that were speciﬁcally designed for point sampled geometry and are based on local surface approximations to achieve high compression rates that outperform previous progressive coders for point-sampled geometry. Moreover we demonstrate that additional point attributes, such as color, which are of great importance for point-sampled geometry, can be well integrated and efﬁciently encoded in this framework.},
	author = {Schnabel, Ruwen and Klein, Reinhard},
	langid = {english},
	keywords = {fundamentals},
	file = {Schnabel and Klein - Octree-based Point-Cloud Compression.pdf:/home/hugop/Zotero/storage/L3QHFUGK/Schnabel and Klein - Octree-based Point-Cloud Compression.pdf:application/pdf},
}

@article{gabriel_zachmann_geometric_2002,
	title = {Geometric Data Structures for Computer Graphics},
	abstract = {The goal of this tutorial is to present a wide range of geometric data structures, algorithms and techniques from computational geometry to computer graphics practitioners. To achieve this goal we introduce several data structures, discuss their complexity, point out construction schemes and the corresponding performance and present standard applications in two and three dimensions.},
	journaltitle = {The Eurographics Association},
	author = {{Gabriel Zachmann} and {Elmar Langetepe}},
	date = {2002},
	langid = {english},
	keywords = {fundamentals},
	file = {Zachmann - Geometric Data Structures for Computer Graphics.pdf:/home/hugop/Zotero/storage/TRQWNE97/Zachmann - Geometric Data Structures for Computer Graphics.pdf:application/pdf},
}

@inproceedings{beazley_swig_1996,
	title = {{SWIG}: An Easy to Use Tool for Integrating Scripting Languages with C and C++},
	volume = {43},
	url = {https://www.semanticscholar.org/paper/SWIG%3A-An-Easy-to-Use-Tool-for-Integrating-Scripting-Beazley/29c88180b5b146e92d94fa73a581b3b8acaf58ee},
	shorttitle = {{SWIG}},
	abstract = {I present {SWIG} (Simplified Wrapper and Interface Generator), a program development tool that automatically generates the bindings between C/C++ code and common scripting languages including Tcl, Python, Perl and Guile. {SWIG} supports most C/C++ datatypes including pointers, structures, and classes. Unlike many other approaches, {SWIG} uses {ANSI} C/C++ declarations and requires the user to make virtually no modifications to the underlying C code. In addition, {SWIG} automatically produces documentation in {HTML}, {LaTeX}, or {ASCII} format. {SWIG} has been primarily designed for scientists, engineers, and application developers who would like to use scripting languages with their C/C++ programs without worrying about the underlying implementation details of each language or using a complicated software development tool. This paper concentrates on {SWIG}'s use with Tcl/Tk.},
	pages = {74},
	booktitle = {Tcl/Tk Workshop},
	author = {Beazley, D.},
	urldate = {2024-04-02},
	date = {1996-07-10},
	keywords = {fundamentals},
	file = {SWIG \: An Easy to Use Tool For Integrating Scriptint Languages with C and C++:/home/hugop/Zotero/storage/RXPS93DY/index.html:text/html},
}

@article{duda_use_1972,
	title = {Use of the Hough transformation to detect lines and curves in pictures},
	volume = {15},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/361237.361242},
	doi = {10.1145/361237.361242},
	abstract = {Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.},
	pages = {11--15},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Duda, Richard O. and Hart, Peter E.},
	urldate = {2024-04-18},
	date = {1972-01-01},
	keywords = {fundamentals, colinear points, curve detection, Hough transformation, line detection, pattern recognition, picture processing, point-line transformation},
	file = {Full Text PDF:/home/hugop/Zotero/storage/9TCMDK8F/Duda and Hart - 1972 - Use of the Hough transformation to detect lines an.pdf:application/pdf},
}

@article{fischler_random_1981,
	title = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
	volume = {24},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/358669.358692},
	doi = {10.1145/358669.358692},
	shorttitle = {Random sample consensus},
	abstract = {A new paradigm, Random Sample Consensus ({RANSAC}), for fitting a model to experimental data is introduced. {RANSAC} is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of {RANSAC} to the Location Determination Problem ({LDP}): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a {RANSAC} requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the {LDP} under difficult viewing},
	pages = {381--395},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	urldate = {2024-04-18},
	date = {1981-06-01},
	keywords = {model fitting, fundamentals, automated cartography, camera calibration, image matching, location determination, scene analysis},
	file = {Full Text PDF:/home/hugop/Zotero/storage/3G456BLJ/Fischler and Bolles - 1981 - Random sample consensus a paradigm for model fitt.pdf:application/pdf},
}

@patent{hough_method_1962,
	title = {Method and means for recognizing complex patterns},
	url = {https://patents.google.com/patent/US3069654A/en},
	holder = {Individual},
	type = {patentus},
	number = {3069654A},
	author = {Hough, Paul V. C.},
	urldate = {2024-04-18},
	date = {1962-12-18},
	keywords = {fundamentals, framelet, line, microsecond, pulse, segment},
	file = {Full Text PDF:/home/hugop/Zotero/storage/DAK4WK3V/Hough - 1962 - Method and means for recognizing complex patterns.pdf:application/pdf},
}

@inproceedings{woodford_contraction_2012,
	location = {Berlin, Heidelberg},
	title = {Contraction Moves for Geometric Model Fitting},
	isbn = {978-3-642-33786-4},
	doi = {10.1007/978-3-642-33786-4_14},
	abstract = {This paper presents a new class of moves, called α-expansion-contraction, which generalizes α-expansion graph cuts for multi-label energy minimization problems. The new moves are particularly useful for optimizing the assignments in model fitting frameworks whose energies include Label Cost ({LC}), as well as Markov Random Field ({MRF}) terms. These problems benefit from the contraction moves’ greater scope for removing instances from the model, reducing label costs. We demonstrate this effect on the problem of fitting sets of geometric primitives to point cloud data, including real-world point clouds containing millions of points, obtained by multi-view reconstruction.},
	pages = {181--194},
	booktitle = {Computer Vision – {ECCV} 2012},
	publisher = {Springer},
	author = {Woodford, Oliver J. and Pham, Minh-Tri and Maki, Atsuto and Gherardi, Riccardo and Perbet, Frank and Stenger, Björn},
	editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	date = {2012},
	langid = {english},
	keywords = {fundamentals},
	file = {Full Text PDF:/home/hugop/Zotero/storage/2JZXRWE7/Woodford et al. - 2012 - Contraction Moves for Geometric Model Fitting.pdf:application/pdf},
}

@inproceedings{milgram_augmented_1995,
	location = {Boston, {MA}},
	title = {Augmented reality: a class of displays on the reality-virtuality continuum},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=981543},
	doi = {10.1117/12.197321},
	shorttitle = {Augmented reality},
	abstract = {In this paper we discuss Augmented Reality ({AR}) displays in a general sense, within the context of a Reality-Virtuality ({RV}) continuum, encompassing a large class of "Mixed Reality" ({MR}) displays, which also includes Augmented Virtuality ({AV}). {MR} displays are defined by means of seven examples of existing display concepts in which real objects and virtual objects are juxtaposed. Essential factors which distinguish different Mixed Reality display systems from each other are presented, first by means of a table in which the nature of the underlying scene, how it is viewed, and the observer's reference to it are compared, and then by means of a three dimensional taxonomic framework, comprising: Extent of World Knowledge ({EWK}), Reproduction Fidelity ({RF}) and Extent of Presence Metaphor ({EPM}). A principal objective of the taxonomy is to clarify terminology issues and to provide a framework for classifying research across different disciplines.},
	eventtitle = {Photonics for Industrial Applications},
	pages = {282--292},
	author = {Milgram, Paul and Takemura, Haruo and Utsumi, Akira and Kishino, Fumio},
	editor = {Das, Hari},
	urldate = {2024-04-23},
	date = {1995-12-21},
	langid = {english},
	keywords = {fundamentals},
	file = {Milgram et al. - 1995 - Augmented reality a class of displays on the real.pdf:/home/hugop/Zotero/storage/JUUSHE6M/Milgram et al. - 1995 - Augmented reality a class of displays on the real.pdf:application/pdf},
}

@article{graham_efficient_1972,
	title = {An efficient algorith for determining the convex hull of a finite planar set},
	volume = {1},
	issn = {0020-0190},
	url = {https://www.sciencedirect.com/science/article/pii/0020019072900452},
	doi = {10.1016/0020-0190(72)90045-2},
	pages = {132--133},
	number = {4},
	journaltitle = {Information Processing Letters},
	shortjournal = {Information Processing Letters},
	author = {Graham, R. L.},
	urldate = {2024-04-26},
	date = {1972-06-01},
	file = {Graham.pdf:/home/hugop/Zotero/storage/HPLRTK8A/Graham.pdf:application/pdf;ScienceDirect Snapshot:/home/hugop/Zotero/storage/4GWYA4CN/0020019072900452.html:text/html},
}

@article{andrew_another_1979,
	title = {Another efficient algorithm for convex hulls in two dimensions},
	volume = {9},
	issn = {0020-0190},
	url = {https://www.sciencedirect.com/science/article/pii/0020019079900723},
	doi = {10.1016/0020-0190(79)90072-3},
	pages = {216--219},
	number = {5},
	journaltitle = {Information Processing Letters},
	shortjournal = {Information Processing Letters},
	author = {Andrew, A. M.},
	urldate = {2024-04-26},
	date = {1979-12-16},
	keywords = {Convex hull},
	file = {ScienceDirect Snapshot:/home/hugop/Zotero/storage/YKGVKTUL/0020019079900723.html:text/html},
}

@book{glassner_graphics_1994,
	location = {San Diego},
	edition = {Reissue edition},
	title = {Graphics Gems},
	isbn = {978-0-12-286166-6},
	abstract = {"The {GRAPHICS} {GEMS} Series" was started in 1990 by Andrew Glassner. The vision and purpose of the Series was - and still is - to provide tips, techniques, and algorithms for graphics programmers. All of the gems are written by programmers who work in the field and are motivated by a common desire to share interesting ideas and tools with their colleagues. Each volume provides a new set of innovative solutions to a variety of programming problems.},
	pagetotal = {864},
	publisher = {Morgan Kaufmann},
	author = {Glassner, Andrew S.},
	date = {1994-01-05},
	file = {AP Professional (Firm) and Electrographics Electronic Imaging (Firm) - 1995 - The AP Professional graphics CD-ROM.pdf:/home/hugop/Zotero/storage/YCU4PUDR/AP Professional (Firm) and Electrographics Electronic Imaging (Firm) - 1995 - The AP Professional graphics CD-ROM.pdf:application/pdf},
}

@book{samet_design_1989,
	location = {Reading, Mass},
	title = {The Design and Analysis of Spatial Data Structures},
	isbn = {978-0-201-50255-8},
	pagetotal = {510},
	publisher = {Addison Wesley},
	author = {Samet, Hanan},
	date = {1989-08-01},
	file = {The Design And Analysis Of Spatial Data Structures - Hanan Samet.pdf:/home/hugop/Zotero/storage/BWL5MTN9/The Design And Analysis Of Spatial Data Structures - Hanan Samet.pdf:application/pdf},
}

@article{finkel_quad_1974,
	title = {Quad trees a data structure for retrieval on composite keys},
	volume = {4},
	issn = {1432-0525},
	url = {https://doi.org/10.1007/BF00288933},
	doi = {10.1007/BF00288933},
	abstract = {The quad tree is a data structure appropriate for storing information to be retrieved on composite keys. We discuss the specific case of two-dimensional retrieval, although the structure is easily generalised to arbitrary dimensions. Algorithms are given both for staightforward insertion and for a type of balanced insertion into quad trees. Empirical analyses show that the average time for insertion is logarithmic with the tree size. An algorithm for retrieval within regions is presented along with data from empirical studies which imply that searching is reasonably efficient. We define an optimized tree and present an algorithm to accomplish optimization in n log n time. Searching is guaranteed to be fast in optimized trees. Remaining problems include those of deletion from quad trees and merging of quad trees, which seem to be inherently difficult operations.},
	pages = {1--9},
	number = {1},
	journaltitle = {Acta Informatica},
	shortjournal = {Acta Informatica},
	author = {Finkel, R. A. and Bentley, J. L.},
	urldate = {2024-05-20},
	date = {1974-03-01},
	langid = {english},
	keywords = {Communication Network, Data Structure, Empirical Study, Information System, Operating System},
	file = {Full Text PDF:/home/hugop/Zotero/storage/3ZJU87KV/Finkel and Bentley - 1974 - Quad Trees A Data Structure for Retrieval on Comp.pdf:application/pdf},
}

@article{klinker_augmented_nodate,
	title = {Augmented Reality {II} - Camera Calibration -},
	author = {Klinker, Gudrun},
	langid = {english},
	file = {Klinker - Augmented Reality II - Camera Calibration -.pdf:/home/hugop/Zotero/storage/4GBKBHEE/Klinker - Augmented Reality II - Camera Calibration -.pdf:application/pdf},
}

@incollection{mehler-bicher_augmented_2022,
	title = {Augmented Reality: Theorie und Praxis},
	rights = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-3-11-075650-0},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110756500/html},
	shorttitle = {Augmented Reality},
	abstract = {As an extension of the real world and as virtual objects, augmented reality is a typical example of an innovative technology. This book looks at examples to reveal the potential that this technology has for various areas of application. The new edition incorporates more best-practice insights. It also addresses the interplay between augmented reality and current technologies like big data and artificial intelligence.},
	booktitle = {Augmented Reality},
	publisher = {De Gruyter Oldenbourg},
	author = {Mehler-Bicher, Anett and Steiger, Lothar},
	urldate = {2024-05-21},
	date = {2022-05-09},
	langid = {german},
	doi = {10.1515/9783110756500},
	keywords = {Avatar, Interface, Living Game, Marker, Rendering, Tracking},
	file = {10.1515_9783110756500.pdf:/home/hugop/Zotero/storage/GF4C42W5/10.1515_9783110756500.pdf:application/pdf;Full Text PDF:/home/hugop/Zotero/storage/2U27WC6U/Mehler-Bicher and Steiger - 2022 - Augmented Reality Theorie und Praxis.pdf:application/pdf},
}

@collection{dorner_virtual_2019,
	location = {Berlin, Heidelberg},
	title = {Virtual und Augmented Reality ({VR}/{AR}): Grundlagen und Methoden der Virtuellen und Augmentierten Realität},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-662-58860-4 978-3-662-58861-1},
	url = {http://link.springer.com/10.1007/978-3-662-58861-1},
	shorttitle = {Virtual und Augmented Reality ({VR}/{AR})},
	publisher = {Springer},
	editor = {Dörner, Ralf and Broll, Wolfgang and Grimm, Paul and Jung, Bernhard},
	urldate = {2024-05-21},
	date = {2019},
	langid = {german},
	doi = {10.1007/978-3-662-58861-1},
	keywords = {Augmented Reality, Augmentierte Realität, Buch Virtuelle und Augmentierte Realität, Computer Graphics, Computer Vision, Computergrafik, Grundlagen, Human Computer Interaction, Interactive Systems, Selbststudium, Studierende, Technologie, Virtual Reality, Virtual und Augmented Reality ({VR} / {AR}), Virtuelle Realität, {VR}},
	file = {Dörner et al. - 2019 - Virtual und Augmented Reality (VRAR) Grundlagen .pdf:/home/hugop/Zotero/storage/3UNK5C6Z/Dörner et al. - 2019 - Virtual und Augmented Reality (VRAR) Grundlagen .pdf:application/pdf},
}

@article{szeliski_computer_nodate,
	title = {Computer Vision: Algorithms and Applications, 2nd Edition},
	author = {Szeliski, Richard},
	langid = {english},
	file = {Szeliski - Computer Vision Algorithms and Applications, 2nd .pdf:/home/hugop/Zotero/storage/PHYTJX8L/Szeliski - Computer Vision Algorithms and Applications, 2nd .pdf:application/pdf},
}
